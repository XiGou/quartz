<!DOCTYPE html>
<html lang="en"><head><title>What Is ChatGPT Doing â€¦ and Why Does It Work</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto Mono&amp;family=Noto Sans SC:wght@400;700&amp;family=Noto Sans SC:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="What Is ChatGPT Doing â€¦ and Why Does It Work"/><meta property="og:description" content="..."/><meta property="og:image" content="https://quartz.19960312.xyz/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../../static/icon.png"/><meta name="description" content="..."/><meta name="generator" content="Quartz"/><link href="../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Collections/Repost/CS/What-Is-ChatGPT-Doing-â€¦-and-Why-Does-It-Work"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title"><a href="../../..">ğŸ˜„ Joker's Digital Garden</a></h1><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div><div class="recent-notes desktop-only"><h3>Recent</h3><ul class="recent-ul"><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Travels" class="internal">Others</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Shenzhen" class="internal">Shenzhen</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Shanghai" class="internal">Shanghai</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Guangzhou" class="internal">Guangzhou</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Beijing" class="internal">Beijing</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li></ul><p><a href="../../../tags">See 135 more â†’</a></p></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../">Home</a><p> â¯ </p></div><div class="breadcrumb-element"><a href="../../../Collections/">Collections</a><p> â¯ </p></div><div class="breadcrumb-element"><a href="../../../Collections/Repost/">Repost</a><p> â¯ </p></div><div class="breadcrumb-element"><a href="../../../Collections/Repost/CS/">CS</a><p> â¯ </p></div><div class="breadcrumb-element"><a href>What Is ChatGPT Doing â€¦ and Why Does It Work</a></div></nav><h1 class="article-title">What Is ChatGPT Doing â€¦ and Why Does It Work</h1><p show-comma="true" class="content-meta"><span>May 03, 2024</span><span>94 min read</span></p><div class="recent-notes mobile-only"><h3>Recent</h3><ul class="recent-ul"><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../Travels/Travels" class="internal">Others</a></h3></div><p class="meta">May 03, 2024</p><ul class="tags"></ul></div></li></ul><p><a href="../../../tags">See 139 more â†’</a></p></div></div></div><article class="popover-hint"><p><img src="https://content.wolfram.com/sites/43/2023/02/hero3-chat-exposition.png" alt title="What Is ChatGPT Doing ... and Why Does It Work?"/></p>
<h2 id="its-just-adding-one-word-at-a-time">Itâ€™s Just Adding One Word at a Time<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#its-just-adding-one-word-at-a-time" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>That <a href="https://chat.openai.com/" class="external">ChatGPT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> can automatically generate something that reads even superficially like human-written text is remarkable, and unexpected. But how does it do it? And why does it work? My purpose here is to give a rough outline of whatâ€™s going on inside ChatGPTâ€”and then to explore why it is that it can do so well in producing what we might consider to be meaningful text. I should say at the outset that Iâ€™m going to focus on the big picture of whatâ€™s going onâ€”and while Iâ€™ll mention some engineering details, I wonâ€™t get deeply into them. (And the essence of what Iâ€™ll say applies just as well to other current â€œlarge language modelsâ€ [LLMs] as to ChatGPT.)</p>
<p>The first thing to explain is that what ChatGPT is always fundamentally trying to do is to produce a â€œreasonable continuationâ€ of whatever text itâ€™s got so far, where by â€œreasonableâ€ we mean â€œwhat one might expect someone to write after seeing what people have written on billions of webpages, etc.â€</p>
<p>So letâ€™s say weâ€™ve got the text â€œ<em>The best thing about AI is its ability to</em>â€. Imagine scanning billions of pages of human-written text (say on the web and in digitized books) and finding all instances of this textâ€”then seeing what word comes next what fraction of the time. ChatGPT effectively does something like this, except that (as Iâ€™ll explain) it doesnâ€™t look at literal text; it looks for things that in a certain sense â€œmatch in meaningâ€. But the end result is that it produces a ranked list of words that might follow, together with â€œprobabilitiesâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img1.png" alt/></p>
<p>And the remarkable thing is that when ChatGPT does something like write an essay what itâ€™s essentially doing is just asking over and over again â€œgiven the text so far, what should the next word be?â€â€”and each time adding a word. (More precisely, as Iâ€™ll explain, itâ€™s adding a â€œtokenâ€, which could be just a part of a word, which is why it can sometimes â€œmake up new wordsâ€.)</p>
<p>But, OK, at each step it gets a list of words with probabilities. But which one should it actually pick to add to the essay (or whatever) that itâ€™s writing? One might think it should be the â€œhighest-rankedâ€ word (i.e. the one to which the highest â€œprobabilityâ€ was assigned). But this is where a bit of voodoo begins to creep in. Because for some reasonâ€”that maybe one day weâ€™ll have a scientific-style understanding ofâ€”if we always pick the highest-ranked word, weâ€™ll typically get a very â€œflatâ€ essay, that never seems to â€œshow any creativityâ€ (and even sometimes repeats word for word). But if sometimes (at random) we pick lower-ranked words, we get a â€œmore interestingâ€ essay.</p>
<p>The fact that thereâ€™s randomness here means that if we use the same prompt multiple times, weâ€™re likely to get different essays each time. And, in keeping with the idea of voodoo, thereâ€™s a particular so-called â€œtemperatureâ€ parameter that determines how often lower-ranked words will be used, and for essay generation, it turns out that a â€œtemperatureâ€ of 0.8 seems best. (Itâ€™s worth emphasizing that thereâ€™s no â€œtheoryâ€ being used here; itâ€™s just a matter of whatâ€™s been found to work in practice. And for example the concept of â€œtemperatureâ€ is there because exponential distributions <a href="https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/#textbook-thermodynamics" class="external">familiar from statistical physics<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> happen to be being used, but thereâ€™s no â€œphysicalâ€ connectionâ€”at least so far as we know.)</p>
<p>Before we go on I should explain that for purposes of exposition Iâ€™m mostly not going to use the <a href="https://openai.com/blog/chatgpt/" class="external">full system thatâ€™s in ChatGPT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>; instead Iâ€™ll usually work with a simpler <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/" class="external">GPT-2 system<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, which has the nice feature that itâ€™s small enough to be able to run on a standard desktop computer. And so for essentially everything I show Iâ€™ll be able to include explicit <a href="https://www.wolfram.com/language/" class="external">Wolfram Language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> code that you can immediately run on your computer. (Click any picture here to copy the code behind it.)</p>
<p>For example, hereâ€™s how to get the table of probabilities above. First, we have to <a href="https://resources.wolframcloud.com/NeuralNetRepository" class="external">retrieve the underlying â€œlanguage modelâ€ neural net<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img2.png" alt/></p>
<p>Later on, weâ€™ll look inside this neural net, and talk about how it works. But for now we can just apply this â€œnet modelâ€ as a black box to our text so far, and ask for the top 5 words by probability that the model says should follow:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img3.png" alt/></p>
<p>This takes that result and makes it into an explicit formatted â€œ<a href="https://www.wolfram.com/language/elementary-introduction/2nd-ed/45-datasets.html" class="external">dataset<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img4.png" alt/></p>
<p>Hereâ€™s what happens if one repeatedly â€œapplies the modelâ€â€”at each step adding the word that has the top probability (specified in this code as the â€œdecisionâ€ from the model):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img5.png" alt/></p>
<p>What happens if one goes on longer? In this (â€œzero temperatureâ€) case what comes out soon gets rather confused and repetitive:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img6.png" alt/></p>
<p>But what if instead of always picking the â€œtopâ€ word one sometimes randomly picks â€œnon-topâ€ words (with the â€œrandomnessâ€ corresponding to â€œtemperatureâ€ 0.8)? Again one can build up text:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img7.png" alt/></p>
<p>And every time one does this, different random choices will be made, and the text will be differentâ€”as in these 5 examples:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img8.png" alt/></p>
<p>Itâ€™s worth pointing out that even at the first step there are a lot of possible â€œnext wordsâ€ to choose from (at temperature 0.8), though their probabilities fall off quite quickly (and, yes, the straight line on this log-log plot corresponds to an <em>n</em><sup>â€“1</sup> <a href="https://www.wolframscience.com/nks/notes-8-8--zipfs-law/" class="external">â€œpower-lawâ€ decay thatâ€™s very characteristic of the general statistics of language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img10.png" alt/></p>
<p>So what happens if one goes on longer? Hereâ€™s a random example. Itâ€™s better than the top-word (zero temperature) case, but still at best a bit weird:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img11A.png" alt/></p>
<p>This was done with the <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/" class="external">simplest GPT-2 model<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (from 2019). With the newer and <a href="https://platform.openai.com/docs/model-index-for-researchers" class="external">bigger GPT-3 models<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> the results are better. Hereâ€™s the top-word (zero temperature) text produced with the same â€œpromptâ€, but with the biggest GPT-3 model:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img12.png" alt/></p>
<p>And hereâ€™s a random example at â€œtemperature 0.8â€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img13.png" alt/></p>
<h2 id="where-do-the-probabilities-come-from">Where Do the Probabilities Come From?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#where-do-the-probabilities-come-from" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>OK, so ChatGPT always picks its next word based on probabilities. But where do those probabilities come from? Letâ€™s start with a simpler problem. Letâ€™s consider generating English text one letter (rather than word) at a time. How can we work out what the probability for each letter should be?</p>
<p>A very minimal thing we could do is just take a sample of English text, and calculate how often different letters occur in it. So, for example, <a href="https://www.wolfram.com/language/elementary-introduction/2nd-ed/34-associations.html#i-8" class="external">this counts letters in the Wikipedia article<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> on â€œcatsâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img14-edit.png" alt/></p>
<p>And this does the same thing for â€œdogsâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img15.png" alt/></p>
<p>The results are similar, but not the same (â€œoâ€ is no doubt more common in the â€œdogsâ€ article because, after all, it occurs in the word â€œdogâ€ itself). Still, if we take a large enough sample of English text we can expect to eventually get at least fairly consistent results:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img16.png" alt/></p>
<p>Hereâ€™s a sample of what we get if we just generate a sequence of letters with these probabilities:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img17.png" alt/></p>
<p>We can break this into â€œwordsâ€ by adding in spaces as if they were letters with a certain probability:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img18.png" alt/></p>
<p>We can do a slightly better job of making â€œwordsâ€ by forcing the distribution of â€œword lengthsâ€ to agree with what it is in English:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img19.png" alt/></p>
<p>We didnâ€™t happen to get any â€œactual wordsâ€ here, but the results are looking slightly better. To go further, though, we need to do more than just pick each letter separately at random. And, for example, we know that if we have a â€œqâ€, the next letter basically has to be â€œuâ€.</p>
<p>Hereâ€™s a plot of the probabilities for letters on their own:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img20.png" alt/></p>
<p>And hereâ€™s a plot that shows the probabilities of pairs of letters (â€œ2-gramsâ€) in typical English text. The possible first letters are shown across the page, the second letters down the page:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img21.png" alt/></p>
<p>And we see here, for example, that the â€œqâ€ column is blank (zero probability) except on the â€œuâ€ row. OK, so now instead of generating our â€œwordsâ€ a single letter at a time, letâ€™s generate them looking at two letters at a time, using these â€œ2-gramâ€ probabilities. Hereâ€™s a sample of the resultâ€”which happens to include a few â€œactual wordsâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img22.png" alt/></p>
<p>With sufficiently much English text we can get pretty good estimates not just for probabilities of single letters or pairs of letters (2-grams), but also for longer runs of letters. And if we generate â€œrandom wordsâ€ with progressively longer <em>n</em>-gram probabilities, we see that they get progressively â€œmore realisticâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img23.png" alt/></p>
<p>But letâ€™s now assumeâ€”more or less as ChatGPT doesâ€”that weâ€™re dealing with whole words, not letters. There are about 40,000 <a href="https://reference.wolfram.com/language/ref/WordList.html" class="external">reasonably commonly used words in English<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And by looking at a large corpus of English text (say a few million books, with altogether a few hundred billion words), we can get an <a href="https://reference.wolfram.com/language/ref/WordFrequencyData.html" class="external">estimate of how common each word is<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And using this we can start generating â€œsentencesâ€, in which each word is independently picked at random, with the same probability that it appears in the corpus. Hereâ€™s a sample of what we get:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img24.png" alt/></p>
<p>Not surprisingly, this is nonsense. So how can we do better? Just like with letters, we can start taking into account not just probabilities for single words but probabilities for pairs or longer <em>n</em>-grams of words. Doing this for pairs, here are 5 examples of what we get, in all cases starting from the word â€œcatâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img25.png" alt/></p>
<p>Itâ€™s getting slightly more â€œsensible lookingâ€. And we might imagine that if we were able to use sufficiently long <em>n</em>-grams weâ€™d basically â€œget a ChatGPTâ€â€”in the sense that weâ€™d get something that would generate essay-length sequences of words with the â€œcorrect overall essay probabilitiesâ€. But hereâ€™s the problem: there just isnâ€™t even close to enough English text thatâ€™s ever been written to be able to deduce those probabilities.</p>
<p>In a <a href="https://commoncrawl.org/" class="external">crawl of the web<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> there might be a few hundred billion words; in books that have been digitized there might be another hundred billion words. But with 40,000 common words, even the number of possible 2-grams is already 1.6 billionâ€”and the number of possible 3-grams is 60 trillion. So thereâ€™s no way we can estimate the probabilities even for all of these from text thatâ€™s out there. And by the time we get to â€œessay fragmentsâ€ of 20 words, the number of possibilities is larger than the number of particles in the universe, so in a sense they could never all be written down.</p>
<p>So what can we do? The big idea is to make a model that lets us estimate the probabilities with which sequences should occurâ€”even though weâ€™ve never explicitly seen those sequences in the corpus of text weâ€™ve looked at. And at the core of ChatGPT is precisely a so-called â€œlarge language modelâ€ (LLM) thatâ€™s been built to do a good job of estimating those probabilities.</p>
<h2 id="what-is-a-model">What Is a Model?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-is-a-model" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Say you want to know (as <a href="https://archive.org/details/bub_gb_49d42xp-USMC/page/404/mode/2up" class="external">Galileo did back in the late 1500s<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) how long itâ€™s going to take a cannon ball dropped from each floor of the Tower of Pisa to hit the ground. Well, you could just measure it in each case and make a table of the results. Or you could do what is the essence of theoretical science: make a model that gives some kind of procedure for computing the answer rather than just measuring and remembering each case.</p>
<p>Letâ€™s imagine we have (somewhat idealized) data for how long the cannon ball takes to fall from various floors:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img26.png" alt/></p>
<p>How do we figure out how long itâ€™s going to take to fall from a floor we donâ€™t explicitly have data about? In this particular case, we can use known laws of physics to work it out. But say all weâ€™ve got is the data, and we donâ€™t know what underlying laws govern it. Then we might make a mathematical guess, like that perhaps we should use a straight line as a model:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img27.png" alt/></p>
<p>We could pick different straight lines. But this is the one thatâ€™s on average closest to the data weâ€™re given. And from this straight line we can estimate the time to fall for any floor.</p>
<p>How did we know to try using a straight line here? At some level we didnâ€™t. Itâ€™s just something thatâ€™s mathematically simple, and weâ€™re used to the fact that lots of data we measure turns out to be well fit by mathematically simple things. We could try something mathematically more complicatedâ€”say <em>a</em> + <em>b</em> <em>x</em> + <em>c</em> <em>x</em><sup>2</sup>â€”and then in this case we do better:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img29.png" alt/></p>
<p>Things can go quite wrong, though. Like hereâ€™s <a href="https://reference.wolfram.com/language/ref/FindFit.html" class="external">the best we can do<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> with <em>a</em> + <em>b</em>/<em>x</em> + <em>c</em> sin(<em>x</em>):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img33.png" alt/></p>
<p>It is worth understanding that thereâ€™s never a â€œmodel-less modelâ€. Any model you use has some particular underlying structureâ€”then a certain set of â€œknobs you can turnâ€ (i.e. parameters you can set) to fit your data. And in the case of ChatGPT, lots of such â€œknobsâ€ are usedâ€”actually, 175 billion of them.</p>
<p>But the remarkable thing is that the underlying structure of ChatGPTâ€”with â€œjustâ€ that many parametersâ€”is sufficient to make a model that computes next-word probabilities â€œwell enoughâ€ to give us reasonable essay-length pieces of text.</p>
<h2 id="models-for-human-like-tasks">Models for Human-Like Tasks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#models-for-human-like-tasks" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The example we gave above involves making a model for numerical data that essentially comes from simple physicsâ€”where weâ€™ve known for several centuries that â€œsimple mathematics appliesâ€. But for ChatGPT we have to make a model of human-language text of the kind produced by a human brain. And for something like that we donâ€™t (at least yet) have anything like â€œsimple mathematicsâ€. So what might a model of it be like?</p>
<p>Before we talk about language, letâ€™s talk about another human-like task: recognizing images. And as a simple example of this, letâ€™s consider images of digits (and, yes, this is a <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/050b1a0a-f43a-4c28-b7e0-72607a918467/" class="external">classic machine learning example<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img34.png" alt/></p>
<p>One thing we could do is get a bunch of sample images for each digit:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img35.png" alt/></p>
<p>Then to find out if an image weâ€™re given as input corresponds to a particular digit we could just do an explicit pixel-by-pixel comparison with the samples we have. But as humans we certainly seem to do something betterâ€”because we can still recognize digits, even when theyâ€™re for example handwritten, and have all sorts of modifications and distortions:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/03/sw021423img36-4.png" alt/></p>
<p>When we made a model for our numerical data above, we were able to take a numerical value <em>x</em> that we were given, and just compute <em>a + b x</em> for particular <em>a</em> and <em>b</em>. So if we treat the gray-level value of each pixel here as some variable <em>x<sub>i</sub></em> is there some function of all those variables thatâ€”when evaluatedâ€”tells us what digit the image is of? It turns out that itâ€™s possible to construct such a function. Not surprisingly, itâ€™s not particularly simple, though. And a typical example might involve perhaps half a million mathematical operations.</p>
<p>But the end result is that if we feed the collection of pixel values for an image into this function, out will come the number specifying which digit we have an image of. Later, weâ€™ll talk about how such a function can be constructed, and the idea of neural nets. But for now letâ€™s treat the function as black box, where we feed in images of, say, handwritten digits (as arrays of pixel values) and we get out the numbers these correspond to:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img38.png" alt/></p>
<p>But whatâ€™s really going on here? Letâ€™s say we progressively blur a digit. For a little while our function still â€œrecognizesâ€ it, here as a â€œ2â€. But soon it â€œloses itâ€, and starts giving the â€œwrongâ€ result:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img39.png" alt/></p>
<p>But why do we say itâ€™s the â€œwrongâ€ result? In this case, we know we got all the images by blurring a â€œ2â€. But if our goal is to produce a model of what humans can do in recognizing images, the real question to ask is what a human would have done if presented with one of those blurred images, without knowing where it came from.</p>
<p>And we have a â€œgood modelâ€ if the results we get from our function typically agree with what a human would say. And the nontrivial scientific fact is that for an image-recognition task like this we now basically know how to construct functions that do this.</p>
<p>Can we â€œmathematically proveâ€ that they work? Well, no. Because to do that weâ€™d have to have a mathematical theory of what we humans are doing. Take the â€œ2â€ image and change a few pixels. We might imagine that with only a few pixels â€œout of placeâ€ we should still consider the image a â€œ2â€. But how far should that go? Itâ€™s a question of <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis#sect-10-7--visual-perception" class="external">human visual perception<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And, yes, the answer would no doubt be different for bees or octopusesâ€”and potentially <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/#alien-views-of-the-ruliad" class="external">utterly different for putative aliens<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</p>
<h2 id="neural-nets">Neural Nets<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#neural-nets" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>OK, so how do our typical models for tasks like <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/" class="external">image recognition<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> actually work? The most popularâ€”and successfulâ€”current approach uses <a href="https://reference.wolfram.com/language/guide/NeuralNetworks.html" class="external">neural nets<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Inventedâ€”in a form remarkably close to their use todayâ€”<a href="https://www.wolframscience.com/nks/notes-10-12--history-of-ideas-about-thinking/" class="external">in the 1940s<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, neural nets can be thought of as simple idealizations of how <a href="https://www.wolframscience.com/nks/notes-10-12--the-brain/" class="external">brains seem to work<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</p>
<p>In human brains there are about 100 billion neurons (nerve cells), each capable of producing an electrical pulse up to perhaps a thousand times a second. The neurons are connected in a complicated net, with each neuron having tree-like branches allowing it to pass electrical signals to perhaps thousands of other neurons. And in a rough approximation, whether any given neuron produces an electrical pulse at a given moment depends on what pulses itâ€™s received from other neuronsâ€”with different connections contributing with different â€œweightsâ€.</p>
<p>When we â€œsee an imageâ€ whatâ€™s happening is that when photons of light from the image fall on (â€œphotoreceptorâ€) cells at the back of our eyes they produce electrical signals in nerve cells. These nerve cells are connected to other nerve cells, and eventually the signals go through a whole sequence of layers of neurons. And itâ€™s in this process that we â€œrecognizeâ€ the image, eventually â€œforming the thoughtâ€ that weâ€™re â€œseeing a 2â€ (and maybe in the end doing something like saying the word â€œtwoâ€ out loud).</p>
<p>The â€œblack-boxâ€ function from the previous section is a â€œmathematicizedâ€ version of such a neural net. It happens to have 11 layers (though only 4 â€œcore layersâ€):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img40A.png" alt/></p>
<p>Thereâ€™s nothing particularly â€œtheoretically derivedâ€ about this neural net; itâ€™s just something thatâ€”<a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/" class="external">back in 1998â€”was constructed as a piece of engineering<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, and found to work. (Of course, thatâ€™s not much different from how we might describe our brains as having been produced through the process of biological evolution.)</p>
<p>OK, but how does a neural net like this â€œrecognize thingsâ€? The key is the <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-7--the-notion-of-attractors" class="external">notion of attractors<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Imagine weâ€™ve got handwritten images of 1â€™s and 2â€™s:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img41.png" alt/></p>
<p>We somehow want all the 1â€™s to â€œbe attracted to one placeâ€, and all the 2â€™s to â€œbe attracted to another placeâ€. Or, put a different way, if an image is somehow â€œ<a href="https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/" class="external">closer to being a 1<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€ than to being a 2, we want it to end up in the â€œ1 placeâ€ and vice versa.</p>
<p>As a straightforward analogy, letâ€™s say we have certain positions in the plane, indicated by dots (in a real-life setting they might be positions of coffee shops). Then we might imagine that starting from any point on the plane weâ€™d always want to end up at the closest dot (i.e. weâ€™d always go to the closest coffee shop). We can represent this by dividing the plane into regions (â€œattractor basinsâ€) separated by idealized â€œwatershedsâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img42.png" alt/></p>
<p>We can think of this as implementing a kind of â€œrecognition taskâ€ in which weâ€™re not doing something like identifying what digit a given image â€œlooks most likeâ€â€”but rather weâ€™re just, quite directly, seeing what dot a given point is closest to. (The â€œVoronoi diagramâ€ setup weâ€™re showing here separates points in 2D Euclidean space; the digit recognition task can be thought of as doing something very similarâ€”but in a 784-dimensional space formed from the gray levels of all the pixels in each image.)</p>
<p>So how do we make a neural net â€œdo a recognition taskâ€? Letâ€™s consider this very simple case:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img43.png" alt/></p>
<p>Our goal is to take an â€œinputâ€ corresponding to a position {<em>x</em>,<em>y</em>}â€”and then to â€œrecognizeâ€ it as whichever of the three points itâ€™s closest to. Or, in other words, we want the neural net to compute a function of {<em>x</em>,<em>y</em>} like:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img44.png" alt/></p>
<p>So how do we do this with a neural net? Ultimately a neural net is a connected collection of idealized â€œneuronsâ€â€”usually arranged in layersâ€”with a simple example being:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img45.png" alt/></p>
<p>Each â€œneuronâ€ is effectively set up to evaluate a simple numerical function. And to â€œuseâ€ the network, we simply feed numbers (like our coordinates <em>x</em> and <em>y</em>) in at the top, then have neurons on each layer â€œevaluate their functionsâ€ and feed the results forward through the networkâ€”eventually producing the final result at the bottom:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img46.png" alt/></p>
<p>In the traditional (biologically inspired) setup each neuron effectively has a certain set of â€œincoming connectionsâ€ from the neurons on the previous layer, with each connection being assigned a certain â€œweightâ€ (which can be a positive or negative number). The value of a given neuron is determined by multiplying the values of â€œprevious neuronsâ€ by their corresponding weights, then adding these up and adding a constantâ€”and finally applying a â€œthresholdingâ€ (or â€œactivationâ€) function. In mathematical terms, if a neuron has inputs <em>x</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub> â€¦} then we compute <em>f</em>[<em>w</em> . <em>x</em> + <em>b</em>], where the weights <em>w</em> and constant <em>b</em> are generally chosen differently for each neuron in the network; the function <em>f</em> is usually the same.</p>
<p>Computing <em>w</em> . <em>x</em> + <em>b</em> is just a matter of matrix multiplication and addition. The â€œactivation functionâ€ <em>f</em> introduces nonlinearity (and ultimately is what leads to nontrivial behavior). Various activation functions commonly get used; here weâ€™ll just use <a href="http://reference.wolfram.com/language/ref/Ramp.html" class="external">Ramp<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (or ReLU):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img48.png" alt/></p>
<p>For each task we want the neural net to perform (or, equivalently, for each overall function we want it to evaluate) weâ€™ll have different choices of weights. (Andâ€”as weâ€™ll discuss laterâ€”these weights are normally determined by â€œtrainingâ€ the neural net using machine learning from examples of the outputs we want.)</p>
<p>Ultimately, every neural net just corresponds to some overall mathematical functionâ€”though it may be messy to write out. For the example above, it would be:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img49.png" alt/></p>
<p>The neural net of ChatGPT also just corresponds to a mathematical function like thisâ€”but effectively with billions of terms.</p>
<p>But letâ€™s go back to individual neurons. Here are some examples of the functions a neuron with two inputs (representing coordinates <em>x</em> and <em>y</em>) can compute with various choices of weights and constants (and <a href="https://reference.wolfram.com/language/ref/Ramp.html" class="external">Ramp<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> as activation function):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img50.png" alt/></p>
<p>But what about the larger network from above? Well, hereâ€™s what it computes:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img51.png" alt/></p>
<p>Itâ€™s not quite â€œrightâ€, but itâ€™s close to the â€œnearest pointâ€ function we showed above.</p>
<p>Letâ€™s see what happens with some other neural nets. In each case, as weâ€™ll explain later, weâ€™re using machine learning to find the best choice of weights. Then weâ€™re showing here what the neural net with those weights computes:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img52.png" alt/></p>
<p>Bigger networks generally do better at approximating the function weâ€™re aiming for. And in the â€œmiddle of each attractor basinâ€ we typically get exactly the answer we want. But <a href="https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/" class="external">at the boundaries<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€”where the neural net â€œhas a hard time making up its mindâ€â€”things can be messier.</p>
<p>With this simple mathematical-style â€œrecognition taskâ€ itâ€™s clear what the â€œright answerâ€ is. But in the problem of recognizing handwritten digits, itâ€™s not so clear. What if someone wrote a â€œ2â€ so badly it looked like a â€œ7â€, etc.? Still, we can ask how a neural net distinguishes digitsâ€”and this gives an indication:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img53.png" alt/></p>
<p>Can we say â€œmathematicallyâ€ how the network makes its distinctions? Not really. Itâ€™s just â€œdoing what the neural net doesâ€. But it turns out that that normally seems to agree fairly well with the distinctions we humans make.</p>
<p>Letâ€™s take a more elaborate example. Letâ€™s say we have images of cats and dogs. And we have a <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/" class="external">neural net thatâ€™s been trained to distinguish them<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Hereâ€™s what it might do on some examples:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img54.png" alt/></p>
<p>Now itâ€™s even less clear what the â€œright answerâ€ is. What about a dog dressed in a cat suit? Etc. Whatever input itâ€™s given the neural net will generate an answer, and in a way reasonably consistent with how humans might. As Iâ€™ve said above, thatâ€™s not a fact we can â€œderive from first principlesâ€. Itâ€™s just something thatâ€™s empirically been found to be true, at least in certain domains. But itâ€™s a key reason why neural nets are useful: that they somehow capture a â€œhuman-likeâ€ way of doing things.</p>
<p>Show yourself a picture of a cat, and ask â€œWhy is that a cat?â€. Maybe youâ€™d start saying â€œWell, I see its pointy ears, etc.â€ But itâ€™s not very easy to explain how you recognized the image as a cat. Itâ€™s just that somehow your brain figured that out. But for a brain thereâ€™s no way (at least yet) to â€œgo insideâ€ and see how it figured it out. What about for an (artificial) neural net? Well, itâ€™s straightforward to see what each â€œneuronâ€ does when you show a picture of a cat. But even to get a basic visualization is usually very difficult.</p>
<p>In the final net that we used for the â€œnearest pointâ€ problem above there are 17 neurons. In the net for recognizing handwritten digits there are 2190. And in the net weâ€™re using to recognize cats and dogs there are 60,650. Normally it would be pretty difficult to visualize what amounts to 60,650-dimensional space. But because this is a network set up to deal with images, many of its layers of neurons are organized into arrays, like the arrays of pixels itâ€™s looking at.</p>
<p>And if we take a typical cat image</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img55.png" alt="Cat" title="Cat"/></p>
<p>then we can represent the states of neurons at the first layer by a collection of derived imagesâ€”many of which we can readily interpret as being things like â€œthe cat without its backgroundâ€, or â€œthe outline of the catâ€:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img56.png" alt/></p>
<p>By the 10th layer itâ€™s harder to interpret whatâ€™s going on:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img57.png" alt/></p>
<p>But in general we might say that the neural net is â€œpicking out certain featuresâ€ (maybe pointy ears are among them), and using these to determine what the image is of. But are those features ones for which we have namesâ€”like â€œpointy earsâ€? Mostly not.</p>
<p>Are our brains using similar features? Mostly we donâ€™t know. But itâ€™s notable that the first few layers of a neural net like the one weâ€™re showing here seem to pick out aspects of images (like edges of objects) that seem to be similar to ones we know are picked out by the first level of visual processing in brains.</p>
<p>But letâ€™s say we want a â€œtheory of cat recognitionâ€ in neural nets. We can say: â€œLook, this particular net does itâ€â€”and immediately that gives us some sense of â€œhow hard a problemâ€ it is (and, for example, how many neurons or layers might be needed). But at least as of now we donâ€™t have a way to â€œgive a narrative descriptionâ€ of what the network is doing. And maybe thatâ€™s because it truly is computationally irreducible, and thereâ€™s no general way to find what it does except by explicitly tracing each step. Or maybe itâ€™s just that we havenâ€™t â€œfigured out the scienceâ€, and identified the â€œnatural lawsâ€ that allow us to summarize whatâ€™s going on.</p>
<p>Weâ€™ll encounter the same kinds of issues when we talk about generating language with ChatGPT. And again itâ€™s not clear whether there are ways to â€œsummarize what itâ€™s doingâ€. But the richness and detail of language (and our experience with it) may allow us to get further than with images.</p>
<h2 id="machine-learning-and-the-training-of-neural-nets">Machine Learning, and the Training of Neural Nets<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#machine-learning-and-the-training-of-neural-nets" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Weâ€™ve been talking so far about neural nets that â€œalready knowâ€ how to do particular tasks. But what makes neural nets so useful (presumably also in brains) is that not only can they in principle do all sorts of tasks, but they can be incrementally â€œtrained from examplesâ€ to do those tasks.</p>
<p>When we make a neural net to distinguish cats from dogs we donâ€™t effectively have to write a program that (say) explicitly finds whiskers; instead we just show lots of examples of whatâ€™s a cat and whatâ€™s a dog, and then have the network â€œmachine learnâ€ from these how to distinguish them.</p>
<p>And the point is that the trained network â€œgeneralizesâ€ from the particular examples itâ€™s shown. Just as weâ€™ve seen above, it isnâ€™t simply that the network recognizes the particular pixel pattern of an example cat image it was shown; rather itâ€™s that the neural net somehow manages to distinguish images on the basis of what we consider to be some kind of â€œgeneral catnessâ€.</p>
<p>So how does neural net training actually work? Essentially what weâ€™re always trying to do is to find weights that make the neural net successfully reproduce the examples weâ€™ve given. And then weâ€™re relying on the neural net to â€œinterpolateâ€ (or â€œgeneralizeâ€) â€œbetweenâ€ these examples in a â€œreasonableâ€ way.</p>
<p>Letâ€™s look at a problem even simpler than the nearest-point one above. Letâ€™s just try to get a neural net to learn the function:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img58.png" alt/></p>
<p>For this task, weâ€™ll need a network that has just one input and one output, like:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img59.png" alt/></p>
<p>But what weights, etc. should we be using? With every possible set of weights the neural net will compute some function. And, for example, hereâ€™s what it does with a few randomly chosen sets of weights:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img60.png" alt/></p>
<p>And, yes, we can plainly see that in none of these cases does it get even close to reproducing the function we want. So how do we find weights that will reproduce the function?</p>
<p>The basic idea is to supply lots of â€œinput â†’ outputâ€ examples to â€œlearn fromâ€â€”and then to try to find weights that will reproduce these examples. Hereâ€™s the result of doing that with progressively more examples:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img61.png" alt/></p>
<p>At each stage in this â€œtrainingâ€ the weights in the network are progressively adjustedâ€”and we see that eventually we get a network that successfully reproduces the function we want. So how do we adjust the weights? The basic idea is at each stage to see â€œhow far away we areâ€ from getting the function we wantâ€”and then to update the weights in such a way as to get closer.</p>
<p>To find out â€œhow far away we areâ€ we compute whatâ€™s usually called a â€œloss functionâ€ (or sometimes â€œcost functionâ€). Here weâ€™re using a simple (L2) loss function thatâ€™s just the sum of the squares of the differences between the values we get, and the true values. And what we see is that as our training process progresses, the loss function progressively decreases (following a certain â€œlearning curveâ€ thatâ€™s different for different tasks)â€”until we reach a point where the network (at least to a good approximation) successfully reproduces the function we want:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img62.png" alt/></p>
<p>Alright, so the last essential piece to explain is how the weights are adjusted to reduce the loss function. As weâ€™ve said, the loss function gives us a â€œdistanceâ€ between the values weâ€™ve got, and the true values. But the â€œvalues weâ€™ve gotâ€ are determined at each stage by the current version of neural netâ€”and by the weights in it. But now imagine that the weights are variablesâ€”say <em>w<sub>i</sub></em>. We want to find out how to adjust the values of these variables to minimize the loss that depends on them.</p>
<p>For example, imagine (in an incredible simplification of typical neural nets used in practice) that we have just two weights <em>w</em><sub>1</sub> and <em>w</em><sub>2</sub>. Then we might have a loss that as a function of <em>w</em><sub>1</sub> and <em>w</em><sub>2</sub> looks like this:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img68.png" alt/></p>
<p>Numerical analysis provides a variety of techniques for finding the minimum in cases like this. But a typical approach is just to progressively follow the path of steepest descent from whatever previous <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub> we had:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img71.png" alt/></p>
<p>Like water flowing down a mountain, all thatâ€™s guaranteed is that this procedure will end up at some local minimum of the surface (â€œa mountain lakeâ€); it might well not reach the ultimate global minimum.</p>
<p>Itâ€™s not obvious that it would be feasible to find the path of the steepest descent on the â€œweight landscapeâ€. But calculus comes to the rescue. As we mentioned above, one can always think of a neural net as computing a mathematical functionâ€”that depends on its inputs, and its weights. But now consider differentiating with respect to these weights. It turns out that the chain rule of calculus in effect lets us â€œunravelâ€ the operations done by successive layers in the neural net. And the result is that we canâ€”at least in some local approximationâ€”â€œinvertâ€ the operation of the neural net, and progressively find weights that minimize the loss associated with the output.</p>
<p>The picture above shows the kind of minimization we might need to do in the unrealistically simple case of just 2 weights. But it turns out that even with many more weights (ChatGPT uses 175 billion) itâ€™s still possible to do the minimization, at least to some level of approximation. And in fact the big breakthrough in â€œdeep learningâ€ that occurred around 2011 was associated with the discovery that in some sense it can be easier to do (at least approximate) minimization when there are lots of weights involved than when there are fairly few.</p>
<p>In other wordsâ€”somewhat counterintuitivelyâ€”it can be easier to solve more complicated problems with neural nets than simpler ones. And the rough reason for this seems to be that when one has a lot of â€œweight variablesâ€ one has a high-dimensional space with â€œlots of different directionsâ€ that can lead one to the minimumâ€”whereas with fewer variables itâ€™s easier to end up getting stuck in a local minimum (â€œmountain lakeâ€) from which thereâ€™s no â€œdirection to get outâ€.</p>
<p>Itâ€™s worth pointing out that in typical cases there are many different collections of weights that will all give neural nets that have pretty much the same performance. And usually in practical neural net training there are lots of random choices madeâ€”that lead to â€œdifferent-but-equivalent solutionsâ€, like these:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img72.png" alt/></p>
<p>But each such â€œdifferent solutionâ€ will have at least slightly different behavior. And if we ask, say, for an â€œextrapolationâ€ outside the region where we gave training examples, we can get dramatically different results:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img73.png" alt/></p>
<p>But which of these is â€œrightâ€? Thereâ€™s really no way to say. Theyâ€™re all â€œconsistent with the observed dataâ€. But they all correspond to different â€œinnateâ€ ways to â€œthink aboutâ€ what to do â€œoutside the boxâ€. And some may seem â€œmore reasonableâ€ to us humans than others.</p>
<h2 id="the-practice-and-lore-of-neural-net-training">The Practice and Lore of Neural Net Training<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-practice-and-lore-of-neural-net-training" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Particularly over the past decade, thereâ€™ve been many advances in the art of training neural nets. And, yes, it is basically an art. Sometimesâ€”especially in retrospectâ€”one can see at least a glimmer of a â€œscientific explanationâ€ for something thatâ€™s being done. But mostly things have been discovered by trial and error, adding ideas and tricks that have progressively built a significant lore about how to work with neural nets.</p>
<p>There are several key parts. First, thereâ€™s the matter of what architecture of neural net one should use for a particular task. Then thereâ€™s the critical issue of how oneâ€™s going to get the data on which to train the neural net. And increasingly one isnâ€™t dealing with training a net from scratch: instead a new net can either directly incorporate another already-trained net, or at least can use that net to generate more training examples for itself.</p>
<p>One might have thought that for every particular kind of task one would need a different architecture of neural net. But whatâ€™s been found is that the same architecture often seems to work even for apparently quite different tasks. At some level this reminds one of the <a href="https://www.wolframscience.com/nks/chap-11--the-notion-of-computation#sect-11-3--the-phenomenon-of-universality" class="external">idea of universal computation<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (and my <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/" class="external">Principle of Computational Equivalence<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>), but, as Iâ€™ll discuss later, I think itâ€™s more a reflection of the fact that the tasks weâ€™re typically trying to get neural nets to do are â€œhuman-likeâ€ onesâ€”and neural nets can capture quite general â€œhuman-like processesâ€.</p>
<p>In earlier days of neural nets, there tended to be the idea that one should â€œmake the neural net do as little as possibleâ€. For example, in <a href="https://reference.wolfram.com/language/ref/SpeechRecognize.html" class="external">converting speech to text<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> it was thought that one should first analyze the audio of the speech, break it into phonemes, etc. But what was found is thatâ€”at least for â€œhuman-like tasksâ€â€”itâ€™s usually better just to try to train the neural net on the â€œend-to-end problemâ€, letting it â€œdiscoverâ€ the necessary intermediate features, encodings, etc. for itself.</p>
<p>There was also the idea that one should introduce complicated individual components into the neural net, to let it in effect â€œexplicitly implement particular algorithmic ideasâ€. But once again, this has mostly turned out not to be worthwhile; instead, itâ€™s better just to deal with very simple components and let them â€œorganize themselvesâ€ (albeit usually in ways we canâ€™t understand) to achieve (presumably) the equivalent of those algorithmic ideas.</p>
<p>Thatâ€™s not to say that there are no â€œstructuring ideasâ€ that are relevant for neural nets. Thus, for example, having <a href="https://reference.wolfram.com/language/ref/ConvolutionLayer.html" class="external">2D arrays of neurons with local connections<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> seems at least very useful in the early stages of processing images. And having patterns of connectivity that concentrate on â€œlooking back in sequencesâ€ seems usefulâ€”as weâ€™ll see laterâ€”in dealing with things like human language, for example in ChatGPT.</p>
<p>But an important feature of neural nets is thatâ€”like computers in generalâ€”theyâ€™re ultimately just dealing with data. And current neural netsâ€”with current approaches to neural net trainingâ€”<a href="https://reference.wolfram.com/language/guide/NetEncoderDecoder.html" class="external">specifically deal with arrays of numbers<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. But in the course of processing, those arrays can be completely rearranged and reshaped. And as an example, <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/" class="external">the network we used for identifying digits above<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> starts with a 2D â€œimage-likeâ€ array, quickly â€œthickeningâ€ to many channels, but then <a href="https://reference.wolfram.com/language/ref/AggregationLayer.html" class="external">â€œconcentrating downâ€ into a 1D array<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that will ultimately contain elements representing the different possible output digits:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img74.png" alt/></p>
<p>But, OK, how can one tell how big a neural net one will need for a particular task? Itâ€™s something of an art. At some level the key thing is to know â€œhow hard the task isâ€. But for human-like tasks thatâ€™s typically very hard to estimate. Yes, there may be a systematic way to do the task very â€œmechanicallyâ€ by computer. But itâ€™s hard to know if there are what one might think of as tricks or shortcuts that allow one to do the task at least at a â€œhuman-like levelâ€ vastly more easily. It might take <a href="https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/" class="external">enumerating a giant game tree<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> to â€œmechanicallyâ€ play a certain game; but there might be a much easier (â€œheuristicâ€) way to achieve â€œhuman-level playâ€.</p>
<p>When oneâ€™s dealing with tiny neural nets and simple tasks one can sometimes explicitly see that one â€œcanâ€™t get there from hereâ€. For example, hereâ€™s the best one seems to be able to do on the task from the previous section with a few small neural nets:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img75.png" alt/></p>
<p>And what we see is that if the net is too small, it just canâ€™t reproduce the function we want. But above some size, it has no problemâ€”at least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if thereâ€™s a â€œsqueezeâ€ in the middle that forces everything to go through a smaller intermediate number of neurons. (Itâ€™s also worth mentioning that â€œno-intermediate-layerâ€â€”or so-called â€œ<a href="https://en.wikipedia.org/wiki/Perceptron" class="external">perceptron<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€â€”networks can only learn essentially linear functionsâ€”but as soon as thereâ€™s even one intermediate layer itâ€™s <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" class="external">always in principle possible<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of <a href="https://reference.wolfram.com/language/ref/BatchNormalizationLayer.html" class="external">regularization or normalization<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.)</p>
<p>OK, so letâ€™s say oneâ€™s settled on a certain neural net architecture. Now thereâ€™s the issue of getting data to train the network with. And many of the practical challenges around neural netsâ€”and machine learning in generalâ€”center on acquiring or preparing the necessary training data. In many cases (â€œsupervised learningâ€) one wants to get explicit examples of inputs and the outputs one is expecting from them. Thus, for example, one might want images tagged by whatâ€™s in them, or some other attribute. And maybe one will have to explicitly go throughâ€”usually with great effortâ€”and do the tagging. But very often it turns out to be possible to piggyback on something thatâ€™s already been done, or use it as some kind of proxy. And so, for example, one might use alt tags that have been provided for images on the web. Or, in a different domain, one might use closed captions that have been created for videos. Orâ€”for language translation trainingâ€”one might use parallel versions of webpages or other documents that exist in different languages.</p>
<p>How much data do you need to show a neural net to train it for a particular task? Again, itâ€™s hard to estimate from first principles. Certainly the requirements can be dramatically reduced by using â€œtransfer learningâ€ to â€œtransfer inâ€ things like lists of important features that have already been learned in another network. But generally neural nets need to â€œsee a lot of examplesâ€ to train well. And at least for some tasks itâ€™s an important piece of neural net lore that the examples can be incredibly repetitive. And indeed itâ€™s a standard strategy to just show a neural net all the examples one has, over and over again. In each of these â€œtraining roundsâ€ (or â€œepochsâ€) the neural net will be in at least a slightly different state, and somehow â€œreminding itâ€ of a particular example is useful in getting it to â€œremember that exampleâ€. (And, yes, perhaps this is analogous to the usefulness of repetition in human memorization.)</p>
<p>But often just repeating the same example over and over again isnâ€™t enough. Itâ€™s also necessary to show the neural net variations of the example. And itâ€™s a feature of neural net lore that those â€œdata augmentationâ€ variations donâ€™t have to be sophisticated to be useful. Just slightly modifying images with basic image processing can make them essentially â€œas good as newâ€ for neural net training. And, similarly, when oneâ€™s run out of actual video, etc. for training self-driving cars, one can go on and just get data from running simulations in a model videogame-like environment without all the detail of actual real-world scenes.</p>
<p>How about something like ChatGPT? Well, it has the nice feature that it can do â€œunsupervised learningâ€, making it much easier to get it examples to train from. Recall that the basic task for ChatGPT is to figure out how to continue a piece of text that itâ€™s been given. So to get it â€œtraining examplesâ€ all one has to do is get a piece of text, and mask out the end of it, and then use this as the â€œinput to train fromâ€â€”with the â€œoutputâ€ being the complete, unmasked piece of text. Weâ€™ll discuss this more later, but the main point is thatâ€”unlike, say, for learning whatâ€™s in imagesâ€”thereâ€™s no â€œexplicit taggingâ€ needed; ChatGPT can in effect just learn directly from whatever examples of text itâ€™s given.</p>
<p>OK, so what about the actual learning process in a neural net? In the end itâ€™s all about determining what weights will best capture the training examples that have been given. And there are all sorts of detailed choices and â€œhyperparameter settingsâ€ (so called because the weights can be thought of as â€œparametersâ€) that can be used to tweak how this is done. There are different <a href="https://reference.wolfram.com/language/ref/CrossEntropyLossLayer.html" class="external">choices of loss function<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (sum of squares, sum of absolute values, etc.). There are different ways to do loss minimization (how far in weight space to move at each step, etc.). And then there are questions like how big a â€œbatchâ€ of examples to show to get each successive estimate of the loss oneâ€™s trying to minimize. And, yes, one can apply machine learning (as we do, for example, in Wolfram Language) to automate machine learningâ€”and to automatically set things like hyperparameters.</p>
<p>But in the end the whole process of training can be characterized by seeing how the loss progressively decreases (as in this <a href="https://reference.wolfram.com/language/ref/NetTrain.html" class="external">Wolfram Language progress monitor for a small training<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img76.png" alt/></p>
<p>And what one typically sees is that the loss decreases for a while, but eventually flattens out at some constant value. If that value is sufficiently small, then the training can be considered successful; otherwise itâ€™s probably a sign one should try changing the network architecture.</p>
<p>Can one tell how long it should take for the â€œlearning curveâ€ to flatten out? Like for so many other things, there seem to be approximate <a href="https://arxiv.org/pdf/2001.08361.pdf" class="external">power-law scaling relationships<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that depend on the size of neural net and amount of data oneâ€™s using. But the general conclusion is that training a neural net is hardâ€”and takes a lot of computational effort. And as a practical matter, the vast majority of that effort is spent doing operations on arrays of numbers, which is what GPUs are good atâ€”which is why neural net training is typically limited by the availability of GPUs.</p>
<p>In the future, will there be fundamentally better ways to train neural netsâ€”or generally do what neural nets do? Almost certainly, I think. The fundamental idea of neural nets is to create a flexible â€œcomputing fabricâ€ out of a large number of simple (essentially identical) componentsâ€”and to have this â€œfabricâ€ be one that can be incrementally modified to learn from examples. In current neural nets, oneâ€™s essentially using the ideas of calculusâ€”applied to real numbersâ€”to do that incremental modification. But itâ€™s increasingly clear that having high-precision numbers doesnâ€™t matter; 8 bits or less might be enough even with current methods.</p>
<p>With <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave" class="external">computational systems like cellular automata<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that basically operate in parallel on many individual bits itâ€™s never been clear <a href="https://content.wolfram.com/sw-publications/2020/07/approaches-complexity-engineering.pdf" class="external">how to do this kind of incremental modification<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, but thereâ€™s no reason to think it isnâ€™t possible. And in fact, much like with the â€œ<a href="https://en.wikipedia.org/wiki/AlexNet" class="external">deep-learning breakthrough of 2012<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€ it may be that such incremental modification will effectively be easier in more complicated cases than in simple ones.</p>
<p>Neural netsâ€”perhaps a bit like brainsâ€”are set up to have an essentially fixed network of neurons, with whatâ€™s modified being the strength (â€œweightâ€) of connections between them. (Perhaps in at least young brains significant numbers of wholly new connections can also grow.) But while this might be a convenient setup for biology, itâ€™s not at all clear that itâ€™s even close to the best way to achieve the functionality we need. And something that involves the equivalent of progressive network rewriting (perhaps reminiscent of our <a href="https://www.wolframphysics.org/" class="external">Physics Project<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) might well ultimately be better.</p>
<p>But even within the framework of existing neural nets thereâ€™s currently a crucial limitation: neural net training as itâ€™s now done is fundamentally sequential, with the effects of each batch of examples being propagated back to update the weights. And indeed with current computer hardwareâ€”even taking into account GPUsâ€”most of a neural net is â€œidleâ€ most of the time during training, with just one part at a time being updated. And in a sense this is because our current computers tend to have memory that is separate from their CPUs (or GPUs). But in brains itâ€™s presumably differentâ€”with every â€œmemory elementâ€ (i.e. neuron) also being a potentially active computational element. And if we could set up our future computer hardware this way it might become possible to do training much more efficiently.</p>
<h2 id="surely-a-network-thats-big-enough-can-do-anything">â€œSurely a Network Thatâ€™s Big Enough Can Do Anything!â€<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#surely-a-network-thats-big-enough-can-do-anything" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The capabilities of something like ChatGPT seem so impressive that one might imagine that if one could just â€œkeep goingâ€ and train larger and larger neural networks, then theyâ€™d eventually be able to â€œdo everythingâ€. And if oneâ€™s concerned with things that are readily accessible to immediate human thinking, itâ€™s quite possible that this is the case. But the lesson of the past several hundred years of science is that there are things that can be figured out by formal processes, but arenâ€™t readily accessible to immediate human thinking.</p>
<p>Nontrivial mathematics is one big example. But the general case is really computation. And ultimately the issue is the phenomenon of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility" class="external">computational irreducibility<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. There are some computations which one might think would take many steps to do, but which can in fact be â€œreducedâ€ to something quite immediate. But the discovery of computational irreducibility implies that this doesnâ€™t always work. And instead there are processesâ€”probably like the one belowâ€”where to work out what happens inevitably requires essentially tracing each computational step:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img77.png" alt/></p>
<p>The kinds of things that we normally do with our brains are presumably specifically chosen to avoid computational irreducibility. It takes special effort to do math in oneâ€™s brain. And itâ€™s in practice largely impossible to â€œthink throughâ€ the steps in the operation of any nontrivial program just in oneâ€™s brain.</p>
<p>But of course for that we have computers. And with computers we can readily do long, computationally irreducible things. And the key point is that thereâ€™s in general no shortcut for these.</p>
<p>Yes, we could memorize lots of specific examples of what happens in some particular computational system. And maybe we could even see some (â€œcomputationally reducibleâ€) patterns that would allow us to do a little generalization. But the point is that computational irreducibility means that we can never guarantee that the unexpected wonâ€™t happenâ€”and itâ€™s only by explicitly doing the computation that you can tell what actually happens in any particular case.</p>
<p>And in the end thereâ€™s just a fundamental tension between learnability and computational irreducibility. Learning involves in effect <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/" class="external">compressing data by leveraging regularities<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. But computational irreducibility implies that ultimately thereâ€™s a limit to what regularities there may be.</p>
<p>As a practical matter, one can imagine building little computational devicesâ€”like cellular automata or Turing machinesâ€”into trainable systems like neural nets. And indeed such devices can serve as good â€œtoolsâ€ for the neural netâ€”like <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/" class="external">Wolfram|Alpha can be a good tool for ChatGPT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. But computational irreducibility implies that one canâ€™t expect to â€œget insideâ€ those devices and have them learn.</p>
<p>Or put another way, thereâ€™s an ultimate tradeoff between capability and trainability: the more you want a system to make â€œtrue useâ€ of its computational capabilities, the more itâ€™s going to show computational irreducibility, and the less itâ€™s going to be trainable. And the more itâ€™s fundamentally trainable, the less itâ€™s going to be able to do sophisticated computation.</p>
<p>(For ChatGPT as it currently is, the situation is actually much more extreme, because the neural net used to generate each token of output is a pure â€œfeed-forwardâ€ network, without loops, and therefore has no ability to do any kind of computation with nontrivial â€œcontrol flowâ€.)</p>
<p>Of course, one might wonder whether itâ€™s actually important to be able to do irreducible computations. And indeed for much of human history it wasnâ€™t particularly important. But our modern technological world has been built on engineering that makes use of at least mathematical computationsâ€”and increasingly also more general computations. And if we look at the natural world, itâ€™s <a href="https://www.wolframscience.com/nks/chap-8--implications-for-everyday-systems/" class="external">full of irreducible computation<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€”that weâ€™re slowly understanding how to emulate and use for our technological purposes.</p>
<p>Yes, a neural net can certainly notice the kinds of regularities in the natural world that we might also readily notice with â€œunaided human thinkingâ€. But if we want to work out things that are in the purview of mathematical or computational science the neural net isnâ€™t going to be able to do itâ€”unless it effectively â€œuses as a toolâ€ an â€œordinaryâ€ computational system.</p>
<p>But thereâ€™s something potentially confusing about all of this. In the past there were plenty of tasksâ€”including writing essaysâ€”that weâ€™ve assumed were somehow â€œfundamentally too hardâ€ for computers. And now that we see them done by the likes of ChatGPT we tend to suddenly think that computers must have become vastly more powerfulâ€”in particular surpassing things they were already basically able to do (like progressively computing the behavior of computational systems like cellular automata).</p>
<p>But this isnâ€™t the right conclusion to draw. Computationally irreducible processes are still computationally irreducible, and are still fundamentally hard for computersâ€”even if computers can readily compute their individual steps. And instead what we should conclude is that tasksâ€”like writing essaysâ€”that we humans could do, but we didnâ€™t think computers could do, are actually in some sense computationally easier than we thought.</p>
<p>In other words, the reason a neural net can be successful in writing an essay is because writing an essay turns out to be a â€œcomputationally shallowerâ€ problem than we thought. And in a sense this takes us closer to â€œhaving a theoryâ€ of how we humans manage to do things like writing essays, or in general deal with language.</p>
<p>If you had a big enough neural net then, yes, you might be able to do whatever humans can readily do. But you wouldnâ€™t capture what the natural world in general can doâ€”or that the tools that weâ€™ve fashioned from the natural world can do. And itâ€™s the use of those toolsâ€”both practical and conceptualâ€”that have allowed us in recent centuries to transcend the boundaries of whatâ€™s accessible to â€œpure unaided human thoughtâ€, and capture for human purposes more of whatâ€™s out there in the physical and computational universe.</p>
<h2 id="the-concept-of-embeddings">The Concept of Embeddings<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-concept-of-embeddings" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Neural netsâ€”at least as theyâ€™re currently set upâ€”are fundamentally based on numbers. So if weâ€™re going to to use them to work on something like text weâ€™ll need a way to <a href="https://reference.wolfram.com/language/guide/NetEncoderDecoder.html" class="external">represent our text with numbers<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And certainly we could start (essentially as ChatGPT does) by just assigning a number to every word in the dictionary. But thereâ€™s an important ideaâ€”thatâ€™s for example central to ChatGPTâ€”that goes beyond that. And itâ€™s the idea of â€œembeddingsâ€. One can think of an embedding as a way to try to represent the â€œessenceâ€ of something by an array of numbersâ€”with the property that â€œnearby thingsâ€ are represented by nearby numbers.</p>
<p>And so, for example, we can think of a word embedding as trying to <a href="https://reference.wolfram.com/language/ref/FeatureSpacePlot.html" class="external">lay out words in a kind of â€œmeaning spaceâ€<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> in which words that are somehow â€œnearby in meaningâ€ appear nearby in the embedding. The actual embeddings that are usedâ€”say in ChatGPTâ€”tend to involve large lists of numbers. But if we project down to 2D, we can show examples of how words are laid out by the embedding:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img78.png" alt/></p>
<p>And, yes, what we see does remarkably well in capturing typical everyday impressions. But how can we construct such an embedding? Roughly the idea is to look at large amounts of text (here 5 billion words from the web) and then see â€œhow similarâ€ the â€œenvironmentsâ€ are in which different words appear. So, for example, â€œalligatorâ€ and â€œcrocodileâ€ will often appear almost interchangeably in otherwise similar sentences, and that means theyâ€™ll be placed nearby in the embedding. But â€œturnipâ€ and â€œeagleâ€ wonâ€™t tend to appear in otherwise similar sentences, so theyâ€™ll be placed far apart in the embedding.</p>
<p>But how does one actually implement something like this using neural nets? Letâ€™s start by talking about embeddings not for words, but for images. We want to find some way to characterize images by lists of numbers in such a way that â€œimages we consider similarâ€ are assigned similar lists of numbers.</p>
<p>How do we tell if we should â€œconsider images similarâ€? Well, if our images are, say, of handwritten digits we might â€œconsider two images similarâ€ if they are of the same digit. Earlier we discussed a neural net that was trained to recognize handwritten digits. And we can think of this neural net as being set up so that in its final output it puts images into 10 different bins, one for each digit.</p>
<p>But what if we â€œinterceptâ€ whatâ€™s going on inside the neural net before the final â€œitâ€™s a â€˜4â€™â€ decision is made? We might expect that inside the neural net there are numbers that characterize images as being â€œmostly 4-like but a bit 2-likeâ€ or some such. And the idea is to pick up such numbers to use as elements in an embedding.</p>
<p>So hereâ€™s the concept. Rather than directly trying to characterize â€œwhat image is near what other imageâ€, we instead consider a well-defined task (in this case digit recognition) for which we can get explicit training dataâ€”then use the fact that in doing this task the neural net implicitly has to make what amount to â€œnearness decisionsâ€. So instead of us ever explicitly having to talk about â€œnearness of imagesâ€ weâ€™re just talking about the concrete question of what digit an image represents, and then weâ€™re â€œleaving it to the neural netâ€ to implicitly determine what that implies about â€œnearness of imagesâ€.</p>
<p>So how in more detail does this work for the digit recognition network? We can think of the network as consisting of 11 successive layers, that we might summarize iconically like this (with activation functions shown as separate layers):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img79.png" alt/></p>
<p>At the beginning weâ€™re feeding into the first layer actual images, represented by 2D arrays of pixel values. And at the endâ€”from the last layerâ€”weâ€™re getting out an array of 10 values, which we can think of saying â€œhow certainâ€ the network is that the image corresponds to each of the digits 0 through 9.</p>
<p>Feed in the image <img src="https://content.wolfram.com/sites/43/2023/02/sw021423number4.png" alt/>and the values of the neurons in that last layer are:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img81.png" alt/></p>
<p>In other words, the neural net is by this point â€œincredibly certainâ€ that this image is a 4â€”and to actually get the output â€œ4â€ we just have to pick out the position of the neuron with the largest value.</p>
<p>But what if we look one step earlier? The very last operation in the network is a so-called <a href="https://reference.wolfram.com/language/ref/SoftmaxLayer.html" class="external">softmax<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> which tries to â€œforce certaintyâ€. But before thatâ€™s been applied the values of the neurons are:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img82.png" alt/></p>
<p>The neuron representing â€œ4â€ still has the highest numerical value. But thereâ€™s also information in the values of the other neurons. And we can expect that this list of numbers can in a sense be used to characterize the â€œessenceâ€ of the imageâ€”and thus to provide something we can use as an embedding. And so, for example, each of the 4â€™s here has a slightly different â€œsignatureâ€ (or â€œfeature embeddingâ€)â€”all very different from the 8â€™s:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img83.png" alt/></p>
<p>Here weâ€™re essentially using 10 numbers to characterize our images. But itâ€™s often better to use much more than that. And for example in our digit recognition network we can get an array of 500 numbers by tapping into the preceding layer. And this is probably a reasonable array to use as an â€œimage embeddingâ€.</p>
<p>If we want to make an explicit visualization of â€œimage spaceâ€ for handwritten digits we need to â€œreduce the dimensionâ€, effectively by projecting the 500-dimensional vector weâ€™ve got into, say, 3D space:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img84.png" alt/></p>
<p>Weâ€™ve just talked about creating a characterization (and thus embedding) for images based effectively on identifying the similarity of images by determining whether (according to our training set) they correspond to the same handwritten digit. And we can do the same thing much more generally for images if we have a training set that identifies, say, which of 5000 common types of object (cat, dog, chair, â€¦) each image is of. And in this way we can make an image embedding thatâ€™s â€œanchoredâ€ by our identification of common objects, but then â€œgeneralizes around thatâ€ according to the behavior of the neural net. And the point is that insofar as that behavior aligns with how we humans perceive and interpret images, this will end up being an embedding that â€œseems right to usâ€, and is useful in practice in doing â€œhuman-judgement-likeâ€ tasks.</p>
<p>OK, so how do we follow the same kind of approach to find embeddings for words? The key is to start from a task about words for which we can readily do training. And the standard such task is â€œword predictionâ€. Imagine weâ€™re given â€œthe ___ catâ€. Based on a large corpus of text (say, the text content of the web), what are the probabilities for different words that might â€œfill in the blankâ€? Or, alternatively, given â€œ___ black ___â€ what are the probabilities for different â€œflanking wordsâ€?</p>
<p>How do we set this problem up for a neural net? Ultimately we have to formulate everything in terms of numbers. And one way to do this is just to assign a unique number to each of the 50,000 or so common words in English. So, for example, â€œtheâ€ might be 914, and â€œ catâ€ (with a space before it) might be 3542. (And these are the actual numbers used by GPT-2.) So for the â€œthe ___ catâ€ problem, our input might be {914, 3542}. What should the output be like? Well, it should be a list of 50,000 or so numbers that effectively give the probabilities for each of the possible â€œfill-inâ€ words. And once again, to find an embedding, we want to â€œinterceptâ€ the â€œinsidesâ€ of the neural net just before it â€œreaches its conclusionâ€â€”and then pick up the list of numbers that occur there, and that we can think of as â€œcharacterizing each wordâ€.</p>
<p>OK, so what do those characterizations look like? Over the past 10 years thereâ€™ve been a sequence of different systems developed (<a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/ConceptNet-Numberbatch-Word-Vectors-V17.06/" class="external">word2vec<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/search/?i=GloVe" class="external">GloVe<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/search/?i=BERT" class="external">BERT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/" class="external">GPT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, â€¦), each based on a different neural net approach. But ultimately all of them take words and characterize them by lists of hundreds to thousands of numbers.</p>
<p>In their raw form, these â€œembedding vectorsâ€ are quite uninformative. For example, hereâ€™s what GPT-2 produces as the raw embedding vectors for three specific words:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img85.png" alt/></p>
<p>If we do things like measure distances between these vectors, then we can find things like â€œnearnessesâ€ of words. Later weâ€™ll discuss in more detail what we might consider the â€œcognitiveâ€ significance of such embeddings. But for now the main point is that we have a way to usefully turn words into â€œneural-net-friendlyâ€ collections of numbers.</p>
<p>But actually we can go further than just characterizing words by collections of numbers; we can also do this for sequences of words, or indeed whole blocks of text. And inside ChatGPT thatâ€™s how itâ€™s dealing with things. It takes the text itâ€™s got so far, and generates an embedding vector to represent it. Then its goal is to find the probabilities for different words that might occur next. And it represents its answer for this as a list of numbers that essentially give the probabilities for each of the 50,000 or so possible words.</p>
<p>(Strictly, ChatGPT does not deal with words, but <a href="https://platform.openai.com/tokenizer" class="external">rather with â€œtokensâ€<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€”convenient linguistic units that might be whole words, or might just be pieces like â€œpreâ€ or â€œingâ€ or â€œizedâ€. Working with tokens makes it easier for ChatGPT to handle rare, compound and non-English words, and, sometimes, for better or worse, to invent new words.)</p>
<h2 id="inside-chatgpt">Inside ChatGPT<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#inside-chatgpt" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>OK, so weâ€™re finally ready to discuss whatâ€™s inside ChatGPT. And, yes, ultimately, itâ€™s a giant neural netâ€”currently a version of the so-called GPT-3 network with 175 billion weights. In many ways this is a neural net very much like the other ones weâ€™ve discussed. But itâ€™s a neural net thatâ€™s particularly set up for dealing with language. And its most notable feature is a piece of neural net architecture called a â€œtransformerâ€.</p>
<p>In the first neural nets we discussed above, every neuron at any given layer was basically connected (at least with some weight) to every neuron on the layer before. But this kind of fully connected network is (presumably) overkill if oneâ€™s working with data that has particular, known structure. And thus, for example, in the early stages of dealing with images, itâ€™s typical to use so-called <a href="https://reference.wolfram.com/language/ref/ConvolutionLayer.html" class="external">convolutional neural nets<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (â€œconvnetsâ€) in which neurons are effectively laid out on a grid analogous to the pixels in the imageâ€”and connected only to neurons nearby on the grid.</p>
<p>The idea of transformers is to do something at least somewhat similar for sequences of tokens that make up a piece of text. But instead of just defining a fixed region in the sequence over which there can be connections, transformers instead introduce the notion of â€œ<a href="https://reference.wolfram.com/language/ref/AttentionLayer.html" class="external">attention<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€â€”and the idea of â€œpaying attentionâ€ more to some parts of the sequence than others. Maybe one day itâ€™ll make sense to just start a generic neural net and do all customization through training. But at least as of now it seems to be critical in practice to â€œmodularizeâ€ thingsâ€”as transformers do, and probably as our brains also do.</p>
<p>OK, so what does ChatGPT (or, rather, the GPT-3 network on which itâ€™s based) actually do? Recall that its overall goal is to continue text in a â€œreasonableâ€ way, based on what itâ€™s seen from the training itâ€™s had (which consists in looking at billions of pages of text from the web, etc.) So at any given point, itâ€™s got a certain amount of textâ€”and its goal is to come up with an appropriate choice for the next token to add.</p>
<p>It operates in three basic stages. First, it takes the sequence of tokens that corresponds to the text so far, and finds an embedding (i.e. an array of numbers) that represents these. Then it operates on this embeddingâ€”in a â€œstandard neural net wayâ€, with values â€œrippling throughâ€ successive layers in a networkâ€”to produce a new embedding (i.e. a new array of numbers). It then takes the last part of this array and generates from it an array of about 50,000 values that turn into probabilities for different possible next tokens. (And, yes, it so happens that there are about the same number of tokens used as there are common words in English, though only about 3000 of the tokens are whole words, and the rest are fragments.)</p>
<p>A critical point is that every part of this pipeline is implemented by a neural network, whose weights are determined by end-to-end training of the network. In other words, in effect nothing except the overall architecture is â€œexplicitly engineeredâ€; everything is just â€œlearnedâ€ from training data.</p>
<p>There are, however, plenty of details in the way the architecture is set upâ€”reflecting all sorts of experience and neural net lore. Andâ€”even though this is definitely going into the weedsâ€”I think itâ€™s useful to talk about some of those details, not least to get a sense of just what goes into building something like ChatGPT.</p>
<p>First comes the embedding module. Hereâ€™s a schematic Wolfram Language representation for it for GPT-2:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img86.png" alt/></p>
<p>The input is a <a href="https://reference.wolfram.com/language/ref/netencoder/SubwordTokens.html" class="external">vector of <em>n</em> tokens<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (represented as in the previous section by integers from 1 to about 50,000). Each of these tokens is converted (by a <a href="https://reference.wolfram.com/language/ref/EmbeddingLayer.html" class="external">single-layer neural net<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) into an embedding vector (of length 768 for GPT-2 and 12,288 for ChatGPTâ€™s GPT-3). Meanwhile, thereâ€™s a â€œsecondary pathwayâ€ that takes the <a href="https://reference.wolfram.com/language/ref/SequenceIndicesLayer.html" class="external">sequence of (integer) positions<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> for the tokens, and from these integers creates another embedding vector. And finally the embedding vectors from the token value and the token position are <a href="https://reference.wolfram.com/language/ref/ThreadingLayer.html" class="external">added together<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€”to produce the final sequence of embedding vectors from the embedding module.</p>
<p>Why does one just add the token-value and token-position embedding vectors together? I donâ€™t think thereâ€™s any particular science to this. Itâ€™s just that various different things have been tried, and this is one that seems to work. And itâ€™s part of the lore of neural nets thatâ€”in some senseâ€”so long as the setup one has is â€œroughly rightâ€ itâ€™s usually possible to home in on details just by doing sufficient training, without ever really needing to â€œunderstand at an engineering levelâ€ quite how the neural net has ended up configuring itself.</p>
<p>Hereâ€™s what the embedding module does, operating on the string <em>hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye</em>:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img87.png" alt/></p>
<p>The elements of the embedding vector for each token are shown down the page, and across the page we see first a run of â€œ<em>hello</em>â€ embeddings, followed by a run of â€œ<em>bye</em>â€ ones. The second array above is the positional embeddingâ€”with its somewhat-random-looking structure being just what â€œhappened to be learnedâ€ (in this case in GPT-2).</p>
<p>OK, so after the embedding module comes the â€œmain eventâ€ of the transformer: a sequence of so-called â€œattention blocksâ€ (12 for GPT-2, 96 for ChatGPTâ€™s GPT-3). Itâ€™s all pretty complicatedâ€”and reminiscent of typical large hard-to-understand engineering systems, or, for that matter, biological systems. But anyway, hereâ€™s a schematic representation of a single â€œattention blockâ€ (for GPT-2):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img88.png" alt/></p>
<p>Within each such attention block there are a collection of â€œattention headsâ€ (12 for GPT-2, 96 for ChatGPTâ€™s GPT-3)â€”each of which operates independently on different chunks of values in the embedding vector. (And, yes, we donâ€™t know any particular reason why itâ€™s a good idea to split up the embedding vector, or what the different parts of it â€œmeanâ€; this is just one of those things thatâ€™s been â€œfound to workâ€.)</p>
<p>OK, so what do the attention heads do? Basically theyâ€™re a way of â€œlooking backâ€ in the sequence of tokens (i.e. in the text produced so far), and â€œpackaging up the pastâ€ in a form thatâ€™s useful for finding the next token. <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#its-just-adding-one-word-at-a-time" class="external">In the first section above<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> we talked about using 2-gram probabilities to pick words based on their immediate predecessors. What the â€œattentionâ€ mechanism in transformers does is to allow â€œattention toâ€ even much earlier wordsâ€”thus potentially capturing the way, say, verbs can refer to nouns that appear many words before them in a sentence.</p>
<p>At a more detailed level, what an attention head does is to recombine chunks in the embedding vectors associated with different tokens, with certain weights. And so, for example, the 12 attention heads in the first attention block (in GPT-2) have the following (â€œlook-back-all-the-way-to-the-beginning-of-the-sequence-of-tokensâ€) patterns of â€œrecombination weightsâ€ for the â€œ<em>hello</em>, <em>bye</em>â€ string above:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img89A.png" alt/></p>
<p>After being processed by the attention heads, the resulting â€œre-weighted embedding vectorâ€ (of length 768 for GPT-2 and length 12,288 for ChatGPTâ€™s GPT-3) is passed through a standard <a href="https://reference.wolfram.com/language/ref/LinearLayer.html" class="external">â€œfully connectedâ€ neural net layer<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Itâ€™s hard to get a handle on what this layer is doing. But hereâ€™s a plot of the 768Ã—768 matrix of weights itâ€™s using (here for GPT-2):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img90A.png" alt/></p>
<p>Taking 64Ã—64 moving averages, some (random-walk-ish) structure begins to emerge:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img91.png" alt/></p>
<p>What determines this structure? Ultimately itâ€™s presumably some â€œneural net encodingâ€ of features of human language. But as of now, what those features might be is quite unknown. In effect, weâ€™re â€œopening up the brain of ChatGPTâ€ (or at least GPT-2) and discovering, yes, itâ€™s complicated in there, and we donâ€™t understand itâ€”even though in the end itâ€™s producing recognizable human language.</p>
<p>OK, so after going through one attention block, weâ€™ve got a new embedding vectorâ€”which is then successively passed through additional attention blocks (a total of 12 for GPT-2; 96 for GPT-3). Each attention block has its own particular pattern of â€œattentionâ€ and â€œfully connectedâ€ weights. Here for GPT-2 are the sequence of attention weights for the â€œhello, byeâ€ input, for the first attention head:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img92A.png" alt/></p>
<p>And here are the (moving-averaged) â€œmatricesâ€ for the fully connected layers:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img93A.png" alt/></p>
<p>Curiously, even though these â€œmatrices of weightsâ€ in different attention blocks look quite similar, the distributions of the sizes of weights can be somewhat different (and are not always Gaussian):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img94A.png" alt/></p>
<p>So after going through all these attention blocks what is the net effect of the transformer? Essentially itâ€™s to transform the original collection of embeddings for the sequence of tokens to a final collection. And the particular way ChatGPT works is then to pick up the last embedding in this collection, and â€œdecodeâ€ it to produce a list of probabilities for what token should come next.</p>
<p>So thatâ€™s in outline whatâ€™s inside ChatGPT. It may seem complicated (not least because of its many inevitably somewhat arbitrary â€œengineering choicesâ€), but actually the ultimate elements involved are remarkably simple. Because in the end what weâ€™re dealing with is just a neural net made of â€œartificial neuronsâ€, each doing the simple operation of taking a collection of numerical inputs, and then combining them with certain weights.</p>
<p>The original input to ChatGPT is an array of numbers (the embedding vectors for the tokens so far), and what happens when ChatGPT â€œrunsâ€ to produce a new token is just that these numbers â€œripple throughâ€ the layers of the neural net, with each neuron â€œdoing its thingâ€ and passing the result to neurons on the next layer. Thereâ€™s no looping or â€œgoing backâ€. Everything just â€œfeeds forwardâ€ through the network.</p>
<p>Itâ€™s a very different setup from a typical computational systemâ€”like a <a href="https://www.wolframscience.com/nks/chap-3--the-world-of-simple-/%20programs/#sect-3-4--turing-machines" class="external">Turing machine<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€”in which results are repeatedly â€œreprocessedâ€ by the same computational elements. Hereâ€”at least in generating a given token of outputâ€”each computational element (i.e. neuron) is used only once.</p>
<p>But there is in a sense still an â€œouter loopâ€ that reuses computational elements even in ChatGPT. Because when ChatGPT is going to generate a new token, it always â€œreadsâ€ (i.e. takes as input) the whole sequence of tokens that come before it, including tokens that ChatGPT itself has â€œwrittenâ€ previously. And we can think of this setup as meaning that ChatGPT doesâ€”at least at its outermost levelâ€”involve a â€œfeedback loopâ€, albeit one in which every iteration is explicitly visible as a token that appears in the text that it generates.</p>
<p>But letâ€™s come back to the core of ChatGPT: the neural net thatâ€™s being repeatedly used to generate each token. At some level itâ€™s very simple: a whole collection of identical artificial neurons. And some parts of the network just consist of (â€œ<a href="https://reference.wolfram.com/language/ref/LinearLayer.html" class="external">fully connected<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€) layers of neurons in which every neuron on a given layer is connected (with some weight) to every neuron on the layer before. But particularly with its transformer architecture, ChatGPT has parts with more structure, in which only specific neurons on different layers are connected. (Of course, one could still say that â€œall neurons are connectedâ€â€”but some just have zero weight.)</p>
<p>In addition, there are aspects of the neural net in ChatGPT that arenâ€™t most naturally thought of as just consisting of â€œhomogeneousâ€ layers. And for exampleâ€”as the iconic summary above indicatesâ€”inside an attention block there are places where â€œmultiple copies are madeâ€ of incoming data, each then going through a different â€œprocessing pathâ€, potentially involving a different number of layers, and only later recombining. But while this may be a convenient representation of whatâ€™s going on, itâ€™s always at least in principle possible to think of â€œdensely filling inâ€ layers, but just having some weights be zero.</p>
<p>If one looks at the longest path through ChatGPT, there are about 400 (core) layers involvedâ€”in some ways not a huge number. But there are millions of neuronsâ€”with a total of 175 billion connections and therefore 175 billion weights. And one thing to realize is that every time ChatGPT generates a new token, it has to do a calculation involving every single one of these weights. Implementationally these calculations can be somewhat organized â€œby layerâ€ into highly parallel array operations that can conveniently be done on GPUs. But for each token thatâ€™s produced, there still have to be 175 billion calculations done (and in the end a bit more)â€”so that, yes, itâ€™s not surprising that it can take a while to generate a long piece of text with ChatGPT.</p>
<p>But in the end, the remarkable thing is that all these operationsâ€”individually as simple as they areâ€”can somehow together manage to do such a good â€œhuman-likeâ€ job of generating text. It has to be emphasized again that (at least so far as we know) thereâ€™s no â€œultimate theoretical reasonâ€ why anything like this should work. And in fact, as weâ€™ll discuss, I think we have to view this as aâ€”potentially surprisingâ€”scientific discovery: that somehow in a neural net like ChatGPTâ€™s itâ€™s possible to capture the essence of what human brains manage to do in generating language.</p>
<h2 id="the-training-of-chatgpt">The Training of ChatGPT<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-training-of-chatgpt" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>OK, so weâ€™ve now given an outline of how ChatGPT works once itâ€™s set up. But how did it get set up? How were all those 175 billion weights in its neural net determined? Basically theyâ€™re the result of very large-scale training, based on a huge corpus of textâ€”on the web, in books, etc.â€”written by humans. As weâ€™ve said, even given all that training data, itâ€™s certainly not obvious that a neural net would be able to successfully produce â€œhuman-likeâ€ text. And, once again, there seem to be detailed pieces of engineering needed to make that happen. But the big surpriseâ€”and discoveryâ€”of ChatGPT is that itâ€™s possible at all. And thatâ€”in effectâ€”a neural net with â€œjustâ€ 175 billion weights can make a â€œreasonable modelâ€ of text humans write.</p>
<p>In modern times, thereâ€™s lots of text written by humans thatâ€™s out there in digital form. The public web has at least several billion human-written pages, with altogether perhaps a trillion words of text. And if one includes non-public webpages, the numbers might be at least 100 times larger. So far, more than 5 million digitized books have been made available (out of 100 million or so that have ever been published), giving another 100 billion or so words of text. And thatâ€™s not even mentioning text derived from speech in videos, etc. (As a personal comparison, <a href="https://www.stephenwolfram.com/publications/" class="external">my total lifetime output of published material<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> has been a bit under 3 million words, and over the <a href="https://writings.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/" class="external">past 30 years Iâ€™ve written<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> about 15 million words of email, and altogether typed perhaps 50 million wordsâ€”and in just the past couple of years Iâ€™ve spoken more than 10 million words on <a href="https://www.stephenwolfram.com/livestreams" class="external">livestreams<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And, yes, Iâ€™ll train a bot from all of that.)</p>
<p>But, OK, given all this data, how does one train a neural net from it? The basic process is very much as we discussed it in the simple examples above. You present a batch of examples, and then you adjust the weights in the network to minimize the error (â€œlossâ€) that the network makes on those examples. The main thing thatâ€™s expensive about â€œback propagatingâ€ from the error is that each time you do this, every weight in the network will typically change at least a tiny bit, and there are just a lot of weights to deal with. (The actual â€œback computationâ€ is typically only a small constant factor harder than the forward one.)</p>
<p>With modern GPU hardware, itâ€™s straightforward to compute the results from batches of thousands of examples in parallel. But when it comes to actually updating the weights in the neural net, current methods require one to do this basically batch by batch. (And, yes, this is probably where actual brainsâ€”with their combined computation and memory elementsâ€”have, for now, at least an architectural advantage.)</p>
<p>Even in the seemingly simple cases of learning numerical functions that we discussed earlier, we found we often had to use millions of examples to successfully train a network, at least from scratch. So how many examples does this mean weâ€™ll need in order to train a â€œhuman-like languageâ€ model? There doesnâ€™t seem to be any fundamental â€œtheoreticalâ€ way to know. But in practice ChatGPT was successfully trained on a few hundred billion words of text.</p>
<p>Some of the text it was fed several times, some of it only once. But somehow it â€œgot what it neededâ€ from the text it saw. But given this volume of text to learn from, how large a network should it require to â€œlearn it wellâ€? Again, we donâ€™t yet have a fundamental theoretical way to say. Ultimatelyâ€”as weâ€™ll discuss further belowâ€”thereâ€™s presumably a certain â€œtotal algorithmic contentâ€ to human language and what humans typically say with it. But the next question is how efficient a neural net will be at implementing a model based on that algorithmic content. And again we donâ€™t knowâ€”although the success of ChatGPT suggests itâ€™s reasonably efficient.</p>
<p>And in the end we can just note that ChatGPT does what it does using a couple hundred billion weightsâ€”comparable in number to the total number of words (or tokens) of training data itâ€™s been given. In some ways itâ€™s perhaps surprising (though empirically observed also in smaller analogs of ChatGPT) that the â€œsize of the networkâ€ that seems to work well is so comparable to the â€œsize of the training dataâ€. After all, itâ€™s certainly not that somehow â€œinside ChatGPTâ€ all that text from the web and books and so on is â€œdirectly storedâ€. Because whatâ€™s actually inside ChatGPT are a bunch of numbersâ€”with a bit less than 10 digits of precisionâ€”that are some kind of distributed encoding of the aggregate structure of all that text.</p>
<p>Put another way, we might ask what the â€œeffective information contentâ€ is of human language and whatâ€™s typically said with it. Thereâ€™s the raw corpus of examples of language. And then thereâ€™s the representation in the neural net of ChatGPT. That representation is very likely far from the â€œalgorithmically minimalâ€ representation (as weâ€™ll discuss below). But itâ€™s a representation thatâ€™s readily usable by the neural net. And in this representation it seems thereâ€™s in the end rather little â€œcompressionâ€ of the training data; it seems on average to basically take only a bit less than one neural net weight to carry the â€œinformation contentâ€ of a word of training data.</p>
<p>When we run ChatGPT to generate text, weâ€™re basically having to use each weight once. So if there are <em>n</em> weights, weâ€™ve got of order <em>n</em> computational steps to doâ€”though in practice many of them can typically be done in parallel in GPUs. But if we need about <em>n</em> words of training data to set up those weights, then from what weâ€™ve said above we can conclude that weâ€™ll need about <em>n</em><sup>2</sup> computational steps to do the training of the networkâ€”which is why, with current methods, one ends up needing to talk about billion-dollar training efforts.</p>
<h2 id="beyond-basic-training">Beyond Basic Training<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#beyond-basic-training" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The majority of the effort in training ChatGPT is spent â€œshowing itâ€ large amounts of existing text from the web, books, etc. But it turns out thereâ€™s anotherâ€”apparently rather importantâ€”part too.</p>
<p>As soon as itâ€™s finished its â€œraw trainingâ€ from the original corpus of text itâ€™s been shown, the neural net inside ChatGPT is ready to start generating its own text, continuing from prompts, etc. But while the results from this may often seem reasonable, they tendâ€”particularly for longer pieces of textâ€”to â€œwander offâ€ in often rather non-human-like ways. Itâ€™s not something one can readily detect, say, by doing traditional statistics on the text. But itâ€™s something that actual humans reading the text easily notice.</p>
<p>And a <a href="https://openai.com/blog/instruction-following/" class="external">key idea in the construction of ChatGPT<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> was to have another step after â€œpassively readingâ€ things like the web: to have actual humans actively interact with ChatGPT, see what it produces, and in effect give it feedback on â€œhow to be a good chatbotâ€. But how can the neural net use that feedback? The first step is just to have humans rate results from the neural net. But then another neural net model is built that attempts to predict those ratings. But now this prediction model can be runâ€”essentially like a loss functionâ€”on the original network, in effect allowing that network to be â€œtuned upâ€ by the human feedback thatâ€™s been given. And the results in practice seem to have a big effect on the success of the system in producing â€œhuman-likeâ€ output.</p>
<p>In general, itâ€™s interesting how little â€œpokingâ€ the â€œoriginally trainedâ€ network seems to need to get it to usefully go in particular directions. One might have thought that to have the network behave as if itâ€™s â€œlearned something newâ€ one would have to go in and run a training algorithm, adjusting weights, and so on.</p>
<p>But thatâ€™s not the case. Instead, it seems to be sufficient to basically tell ChatGPT something one timeâ€”as part of the prompt you giveâ€”and then it can successfully make use of what you told it when it generates text. And once again, the fact that this works is, I think, an important clue in understanding what ChatGPT is â€œreally doingâ€ and how it relates to the structure of human language and thinking.</p>
<p>Thereâ€™s certainly something rather human-like about it: that at least once itâ€™s had all that pre-training you can tell it something just once and it can â€œremember itâ€â€”at least â€œlong enoughâ€ to generate a piece of text using it. So whatâ€™s going on in a case like this? It could be that â€œeverything you might tell it is already in there somewhereâ€â€”and youâ€™re just leading it to the right spot. But that doesnâ€™t seem plausible. Instead, what seems more likely is that, yes, the elements are already in there, but the specifics are defined by something like a â€œtrajectory between those elementsâ€ and thatâ€™s what youâ€™re introducing when you tell it something.</p>
<p>And indeed, much like for humans, if you tell it something bizarre and unexpected that completely doesnâ€™t fit into the framework it knows, it doesnâ€™t seem like itâ€™ll successfully be able to â€œintegrateâ€ this. It can â€œintegrateâ€ it only if itâ€™s basically riding in a fairly simple way on top of the framework it already has.</p>
<p>Itâ€™s also worth pointing out again that there are inevitably â€œalgorithmic limitsâ€ to what the neural net can â€œpick upâ€. Tell it â€œshallowâ€ rules of the form â€œthis goes to thatâ€, etc., and the neural net will most likely be able to represent and reproduce these just fineâ€”and indeed what it â€œalready knowsâ€ from language will give it an immediate pattern to follow. But try to give it rules for an actual â€œdeepâ€ computation that involves many potentially computationally irreducible steps and it just wonâ€™t work. (Remember that at each step itâ€™s always just â€œfeeding data forwardâ€ in its network, never looping except by virtue of generating new tokens.)</p>
<p>Of course, the network can learn the answer to specific â€œirreducibleâ€ computations. But as soon as there are combinatorial numbers of possibilities, no such â€œtable-lookup-styleâ€ approach will work. And so, yes, just like humans, itâ€™s time then for neural nets to â€œreach outâ€ and use actual computational tools. (And, yes, <a href="https://www.wolframalpha.com/" class="external">Wolfram|Alpha<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and <a href="https://www.wolfram.com/language/" class="external">Wolfram Language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> are <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/" class="external">uniquely suitable<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, because theyâ€™ve been built to â€œtalk about things in the worldâ€, just like the language-model neural nets.)</p>
<h2 id="what-really-lets-chatgpt-work">What Really Lets ChatGPT Work?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-really-lets-chatgpt-work" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Human languageâ€”and the processes of thinking involved in generating itâ€”have always seemed to represent a kind of pinnacle of complexity. And indeed itâ€™s seemed somewhat remarkable that human brainsâ€”with their network of a â€œmereâ€ 100 billion or so neurons (and maybe 100 trillion connections) could be responsible for it. Perhaps, one might have imagined, thereâ€™s something more to brains than their networks of neuronsâ€”like some new layer of undiscovered physics. But now with ChatGPT weâ€™ve got an important new piece of information: we know that a pure, artificial neural network with about as many connections as brains have neurons is capable of doing a surprisingly good job of generating human language.</p>
<p>And, yes, thatâ€™s still a big and complicated systemâ€”with about as many neural net weights as there are words of text currently available out there in the world. But at some level it still seems difficult to believe that all the richness of language and the things it can talk about can be encapsulated in such a finite system. Part of whatâ€™s going on is no doubt a reflection of the ubiquitous phenomenon (that first became evident in the <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave" class="external">example of rule 30<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) that computational processes can in effect greatly amplify the apparent complexity of systems even when their underlying rules are simple. But, actually, as we discussed above, neural nets of the kind used in ChatGPT tend to be specifically constructed to restrict the effect of this phenomenonâ€”and the computational irreducibility associated with itâ€”in the interest of making their training more accessible.</p>
<p>So how is it, then, that something like ChatGPT can get as far as it does with language? The basic answer, I think, is that language is at a fundamental level somehow simpler than it seems. And this means that ChatGPTâ€”even with its ultimately straightforward neural net structureâ€”is successfully able to â€œcapture the essenceâ€ of human language and the thinking behind it. And moreover, in its training, ChatGPT has somehow â€œimplicitly discoveredâ€ whatever regularities in language (and thinking) make this possible.</p>
<p>The success of ChatGPT is, I think, giving us evidence of a fundamental and important piece of science: itâ€™s suggesting that we can expect there to be major new â€œlaws of languageâ€â€”and effectively â€œlaws of thoughtâ€â€”out there to discover. In ChatGPTâ€”built as it is as a neural netâ€”those laws are at best implicit. But if we could somehow make the laws explicit, thereâ€™s the potential to do the kinds of things ChatGPT does in vastly more direct, efficientâ€”and transparentâ€”ways.</p>
<p>But, OK, so what might these laws be like? Ultimately they must give us some kind of prescription for how languageâ€”and the things we say with itâ€”are put together. Later weâ€™ll discuss how â€œlooking inside ChatGPTâ€ may be able to give us some hints about this, and how what we know from building computational language suggests a path forward. But first letâ€™s discuss two long-known examples of what amount to â€œlaws of languageâ€â€”and how they relate to the operation of ChatGPT.</p>
<p>The first is the syntax of language. Language is not just a random jumble of words. Instead, there are (fairly) definite <a href="https://www.wolframscience.com/nks/notes-10-12--computer-and-human-languages/" class="external">grammatical rules<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> for how words of different kinds can be put together: in English, for example, nouns can be preceded by adjectives and followed by verbs, but typically two nouns canâ€™t be right next to each other. Such grammatical structure can (at least approximately) be captured by a set of rules that define how what amount to <a href="https://reference.wolfram.com/language/ref/TextStructure.html" class="external">â€œparse treesâ€ can be put together<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img96.png" alt/></p>
<p>ChatGPT doesnâ€™t have any explicit â€œknowledgeâ€ of such rules. But somehow in its training it implicitly â€œdiscoversâ€ themâ€”and then seems to be good at following them. So how does this work? At a â€œbig pictureâ€ level itâ€™s not clear. But to get some insight itâ€™s perhaps instructive to look at a much simpler example.</p>
<p>Consider a â€œlanguageâ€ formed from sequences of (â€™s and )â€™s, with a <a href="https://www.wolframscience.com/nks/notes-7-9--nested-lists/" class="external">grammar that specifies<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that parentheses should always be balanced, as represented by a parse tree like:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img97.png" alt/></p>
<p>Can we train a neural net to produce â€œgrammatically correctâ€ parenthesis sequences? There are various ways to handle sequences in neural nets, but letâ€™s use transformer nets, as ChatGPT does. And given a simple transformer net, we can start feeding it grammatically correct parenthesis sequences as training examples. A subtlety (which actually also appears in ChatGPTâ€™s generation of human language) is that in addition to our â€œcontent tokensâ€ (here â€œ(â€ and â€œ)â€) we have to include an â€œEndâ€ token, thatâ€™s generated to indicate that the output shouldnâ€™t continue any further (i.e. for ChatGPT, that oneâ€™s reached the â€œend of the storyâ€).</p>
<p>If we set up a transformer net with just one attention block with 8 heads and feature vectors of length 128 (ChatGPT also uses feature vectors of length 128, but has 96 attention blocks, each with 96 heads) then it doesnâ€™t seem possible to get it to learn much about parenthesis language. But with 2 attention blocks, the learning process seems to convergeâ€”at least after 10 million or so examples have been given (and, as is common with transformer nets, showing yet more examples just seems to degrade its performance).</p>
<p>So with this network, we can do the analog of what ChatGPT does, and ask for probabilities for what the next token should beâ€”in a parenthesis sequence:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img98.png" alt/></p>
<p>And in the first case, the network is â€œpretty sureâ€ that the sequence canâ€™t end hereâ€”which is good, because if it did, the parentheses would be left unbalanced. In the second case, however, it â€œcorrectly recognizesâ€ that the sequence can end here, though it also â€œpoints outâ€ that itâ€™s possible to â€œstart againâ€, putting down a â€œ(â€, presumably with a â€œ)â€ to follow. But, oops, even with its 400,000 or so laboriously trained weights, it says thereâ€™s a 15% probability to have â€œ)â€ as the next tokenâ€”which isnâ€™t right, because that would necessarily lead to an unbalanced parenthesis.</p>
<p>Hereâ€™s what we get if we ask the network for the highest-probability completions for progressively longer sequences of (â€™s:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img99A.png" alt/></p>
<p>And, yes, up to a certain length the network does just fine. But then it starts failing. Itâ€™s a pretty typical kind of thing to see in a â€œpreciseâ€ situation like this with a neural net (or with machine learning in general). Cases that a human â€œcan solve in a glanceâ€ the neural net can solve too. But cases that require doing something â€œmore algorithmicâ€ (e.g. explicitly counting parentheses to see if theyâ€™re closed) the neural net tends to somehow be â€œtoo computationally shallowâ€ to reliably do. (By the way, even the full current ChatGPT has a hard time correctly matching parentheses in long sequences.)</p>
<p>So what does this mean for things like ChatGPT and the syntax of a language like English? The parenthesis language is â€œaustereâ€â€”and much more of an â€œalgorithmic storyâ€. But in English itâ€™s much more realistic to be able to â€œguessâ€ whatâ€™s grammatically going to fit on the basis of local choices of words and other hints. And, yes, the neural net is much better at thisâ€”even though perhaps it might miss some â€œformally correctâ€ case that, well, humans might miss as well. But the main point is that the fact that thereâ€™s an overall syntactic structure to the languageâ€”with all the regularity that impliesâ€”in a sense limits â€œhow muchâ€ the neural net has to learn. And a key â€œnatural-science-likeâ€ observation is that the transformer architecture of neural nets like the one in ChatGPT seems to successfully be able to learn the kind of nested-tree-like syntactic structure that seems to exist (at least in some approximation) in all human languages.</p>
<p>Syntax provides one kind of constraint on language. But there are clearly more. A sentence like â€œInquisitive electrons eat blue theories for fishâ€ is grammatically correct but isnâ€™t something one would normally expect to say, and wouldnâ€™t be considered a success if ChatGPT generated itâ€”because, well, with the normal meanings for the words in it, itâ€™s basically meaningless.</p>
<p>But is there a general way to tell if a sentence is meaningful? Thereâ€™s no traditional overall theory for that. But itâ€™s something that one can think of ChatGPT as having implicitly â€œdeveloped a theory forâ€ after being trained with billions of (presumably meaningful) sentences from the web, etc.</p>
<p>What might this theory be like? Well, thereâ€™s one tiny corner thatâ€™s basically been known for two millennia, and thatâ€™s <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/" class="external">logic<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And certainly in the syllogistic form in which Aristotle discovered it, logic is basically a way of saying that sentences that follow certain patterns are reasonable, while others are not. Thus, for example, itâ€™s reasonable to say â€œAll X are Y. This is not Y, so itâ€™s not an Xâ€ (as in â€œAll fishes are blue. This is not blue, so itâ€™s not a fish.â€). And just as one can somewhat whimsically imagine that Aristotle discovered syllogistic logic by going (â€œmachine-learning-styleâ€) through lots of examples of rhetoric, so too one can imagine that in the training of ChatGPT it will have been able to â€œdiscover syllogistic logicâ€ by looking at lots of text on the web, etc. (And, yes, while one can therefore expect ChatGPT to produce text that contains â€œcorrect inferencesâ€ based on things like syllogistic logic, itâ€™s a quite different story when it comes to more sophisticated formal logicâ€”and I think one can expect it to fail here for the same kind of reasons it fails in parenthesis matching.)</p>
<p>But beyond the narrow example of logic, what can be said about how to systematically construct (or recognize) even plausibly meaningful text? Yes, there are things like <a href="https://en.wikipedia.org/wiki/Mad_Libs" class="external">Mad Libs<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> that use very specific â€œphrasal templatesâ€. But somehow ChatGPT implicitly has a much more general way to do it. And perhaps thereâ€™s nothing to be said about how it can be done beyond â€œsomehow it happens when you have 175 billion neural net weightsâ€. But I strongly suspect that thereâ€™s a much simpler and stronger story.</p>
<h2 id="meaning-space-and-semantic-laws-of-motion">Meaning Space and Semantic Laws of Motion<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#meaning-space-and-semantic-laws-of-motion" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>We discussed above that inside ChatGPT any piece of text is effectively represented by an array of numbers that we can think of as coordinates of a point in some kind of â€œlinguistic feature spaceâ€. So when ChatGPT continues a piece of text this corresponds to tracing out a trajectory in linguistic feature space. But now we can ask what makes this trajectory correspond to text we consider meaningful. And might there perhaps be some kind of â€œsemantic laws of motionâ€ that defineâ€”or at least constrainâ€”how points in linguistic feature space can move around while preserving â€œmeaningfulnessâ€?</p>
<p>So what is this linguistic feature space like? Hereâ€™s an example of how single words (here, common nouns) might get laid out if we project such a feature space down to 2D:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img100.png" alt/></p>
<p>We saw another example above based on words representing plants and animals. But the point in both cases is that â€œsemantically similar wordsâ€ are placed nearby.</p>
<p>As another example, hereâ€™s how words corresponding to different parts of speech get laid out:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img101-edit.png" alt/></p>
<p>Of course, a given word doesnâ€™t in general just have â€œone meaningâ€ (or necessarily correspond to just one part of speech). And by looking at how sentences containing a word lay out in feature space, one can often â€œtease apartâ€ different meaningsâ€”as in the example here for the word â€œcraneâ€ (bird or machine?):</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img102.png" alt/></p>
<p>OK, so itâ€™s at least plausible that we can think of this feature space as placing â€œwords nearby in meaningâ€ close in this space. But what kind of additional structure can we identify in this space? Is there for example some kind of notion of â€œparallel transportâ€ that would reflect â€œflatnessâ€ in the space? One way to get a handle on that is to look at analogies:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img103.png" alt/></p>
<p>And, yes, even when we project down to 2D, thereâ€™s often at least a â€œhint of flatnessâ€, though itâ€™s certainly not universally seen.</p>
<p>So what about trajectories? We can look at the trajectory that a prompt for ChatGPT follows in feature spaceâ€”and then we can see how ChatGPT continues that:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img104.png" alt/></p>
<p>Thereâ€™s certainly no â€œgeometrically obviousâ€ law of motion here. And thatâ€™s not at all surprising; we fully expect this to be a <a href="https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/#linguistics" class="external">considerably more complicated story<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. And, for example, itâ€™s far from obvious that even if there is a â€œsemantic law of motionâ€ to be found, what kind of embedding (or, in effect, what â€œvariablesâ€) itâ€™ll most naturally be stated in.</p>
<p>In the picture above, weâ€™re showing several steps in the â€œtrajectoryâ€â€”where at each step weâ€™re picking the word that ChatGPT considers the most probable (the â€œzero temperatureâ€ case). But we can also ask what words can â€œcome nextâ€ with what probabilities at a given point:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img105.png" alt/></p>
<p>And what we see in this case is that thereâ€™s a â€œfanâ€ of high-probability words that seems to go in a more or less definite direction in feature space. What happens if we go further? Here are the successive â€œfansâ€ that appear as we â€œmove alongâ€ the trajectory:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img106.png" alt/></p>
<p>Hereâ€™s a 3D representation, going for a total of 40 steps:</p>
<p><img src="https://content.wolfram.com/sites/43/2023/02/sw021423img107.png" alt/></p>
<p>And, yes, this seems like a messâ€”and doesnâ€™t do anything to particularly encourage the idea that one can expect to identify â€œmathematical-physics-likeâ€ â€œsemantic laws of motionâ€ by empirically studying â€œwhat ChatGPT is doing insideâ€. But perhaps weâ€™re just looking at the â€œwrong variablesâ€ (or wrong coordinate system) and if only we looked at the right one, weâ€™d immediately see that ChatGPT is doing something â€œmathematical-physics-simpleâ€ like following geodesics. But as of now, weâ€™re not ready to â€œempirically decodeâ€ from its â€œinternal behaviorâ€ what ChatGPT has â€œdiscoveredâ€ about how human language is â€œput togetherâ€.</p>
<h2 id="semantic-grammar-and-the-power-of-computational-language">Semantic Grammar and the Power of Computational Language<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#semantic-grammar-and-the-power-of-computational-language" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>What does it take to produce â€œmeaningful human languageâ€? In the past, we might have assumed it could be nothing short of a human brain. But now we know it can be done quite respectably by the neural net of ChatGPT. Still, maybe thatâ€™s as far as we can go, and thereâ€™ll be nothing simplerâ€”or more human understandableâ€”that will work. But my strong suspicion is that the success of ChatGPT implicitly reveals an important â€œscientificâ€ fact: that thereâ€™s actually a lot more structure and simplicity to meaningful human language than we ever knewâ€”and that in the end there may be even fairly simple rules that describe how such language can be put together.</p>
<p>As we mentioned above, syntactic grammar gives rules for how words corresponding to things like different parts of speech can be put together in human language. But to deal with meaning, we need to go further. And one version of how to do this is to think about not just a syntactic grammar for language, but also a semantic one.</p>
<p>For purposes of syntax, we identify things like nouns and verbs. But for purposes of semantics, we need â€œfiner gradationsâ€. So, for example, we might identify the concept of â€œmovingâ€, and the concept of an â€œobjectâ€ that â€œmaintains its identity independent of locationâ€. There are endless specific examples of each of these â€œsemantic conceptsâ€. But for the purposes of our semantic grammar, weâ€™ll just have some general kind of rule that basically says that â€œobjectsâ€ can â€œmoveâ€. Thereâ€™s a lot to say about how all this might work (<a href="https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/" class="external">some of which Iâ€™ve said before<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>). But Iâ€™ll content myself here with just a few remarks that indicate some of the potential path forward.</p>
<p>Itâ€™s worth mentioning that even if a sentence is perfectly OK according to the semantic grammar, that doesnâ€™t mean itâ€™s been realized (or even could be realized) in practice. â€œThe elephant traveled to the Moonâ€ would doubtless â€œpassâ€ our semantic grammar, but it certainly hasnâ€™t been realized (at least yet) in our actual worldâ€”though itâ€™s absolutely fair game for a fictional world.</p>
<p>When we start talking about â€œsemantic grammarâ€ weâ€™re soon led to ask â€œWhatâ€™s underneath it?â€ What â€œmodel of the worldâ€ is it assuming? A syntactic grammar is really just about the construction of language from words. But a semantic grammar necessarily engages with some kind of â€œmodel of the worldâ€â€”something that serves as a â€œskeletonâ€ on top of which language made from actual words can be layered.</p>
<p>Until recent times, we might have imagined that (human) language would be the only general way to describe our â€œmodel of the worldâ€. Already a few centuries ago there started to be formalizations of specific kinds of things, based particularly on mathematics. But now thereâ€™s a much more general approach to formalization: <a href="https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/" class="external">computational language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</p>
<p>And, yes, thatâ€™s been my big project over the course of more than four decades (as now embodied in the <a href="https://www.wolfram.com/language/" class="external">Wolfram Language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>): to develop a precise symbolic representation that can talk as broadly as possible about things in the world, as well as abstract things that we care about. And so, for example, we have symbolic representations for <a href="https://reference.wolfram.com/language/ref/entity/City.html" class="external">cities<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and <a href="https://reference.wolfram.com/language/guide/MolecularStructureAndComputation.html" class="external">molecules<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and <a href="https://reference.wolfram.com/language/guide/ImageRepresentation.html" class="external">images<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and <a href="https://reference.wolfram.com/language/guide/NeuralNetworkConstruction.html" class="external">neural networks<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, and we have built-in knowledge about how to compute about those things.</p>
<p>And, after decades of work, weâ€™ve covered a lot of areas in this way. But in the past, we havenâ€™t particularly dealt with â€œ<a href="https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/" class="external">everyday discourse<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>â€. In â€œI bought two pounds of applesâ€ we can <a href="https://reference.wolfram.com/language/ref/entity/Food.html" class="external">readily represent<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (and do nutrition and other computations on) the â€œtwo pounds of applesâ€. But we donâ€™t (quite yet) have a symbolic representation for â€œI boughtâ€.</p>
<p>Itâ€™s all connected to the idea of semantic grammarâ€”and the goal of having a generic symbolic â€œconstruction kitâ€ for concepts, that would give us rules for what could fit together with what, and thus for the â€œflowâ€ of what we might turn into human language.</p>
<p>But letâ€™s say we had this â€œsymbolic discourse languageâ€. What would we do with it? We could start off doing things like generating â€œlocally meaningful textâ€. But ultimately weâ€™re likely to want more â€œglobally meaningfulâ€ resultsâ€”which means â€œcomputingâ€ more about what can actually exist or happen in the world (or perhaps in some consistent fictional world).</p>
<p>Right now in Wolfram Language we have a huge amount of built-in computational knowledge about lots of kinds of things. But for a complete symbolic discourse language weâ€™d have to build in additional â€œcalculiâ€ about general things in the world: if an object moves from A to B and from B to C, then itâ€™s moved from A to C, etc.</p>
<p>Given a symbolic discourse language we might use it to make â€œstandalone statementsâ€. But we can also use it to ask questions about the world, â€œWolfram|Alpha styleâ€. Or we can use it to state things that we â€œwant to make soâ€, presumably with some external actuation mechanism. Or we can use it to make assertionsâ€”perhaps about the actual world, or perhaps about some specific world weâ€™re considering, fictional or otherwise.</p>
<p>Human language is fundamentally imprecise, not least because it isnâ€™t â€œtetheredâ€ to a specific computational implementation, and its meaning is basically defined just by a â€œsocial contractâ€ between its users. But computational language, by its nature, has a certain fundamental precisionâ€”because in the end what it specifies can always be â€œunambiguously executed on a computerâ€. Human language can usually get away with a certain vagueness. (When we say â€œplanetâ€ does it include exoplanets or not, etc.?) But in computational language we have to be precise and clear about all the distinctions weâ€™re making.</p>
<p>Itâ€™s often convenient to leverage ordinary human language in making up names in computational language. But the meanings they have in computational language are necessarily preciseâ€”and might or might not cover some particular connotation in typical human language usage.</p>
<p>How should one figure out the fundamental â€œontologyâ€ suitable for a general symbolic discourse language? Well, itâ€™s not easy. Which is perhaps why little has been done since the primitive beginnings Aristotle made more than two millennia ago. But it really helps that today we now know so much about how to think about the world computationally (and it doesnâ€™t hurt to have a â€œfundamental metaphysicsâ€ from our <a href="https://www.wolframphysics.org/" class="external">Physics Project<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> and the <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/" class="external">idea of the ruliad<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>).</p>
<p>But what does all this mean in the context of ChatGPT? From its training ChatGPT has effectively â€œpieced togetherâ€ a certain (rather impressive) quantity of what amounts to semantic grammar. But its very success gives us a reason to think that itâ€™s going to be feasible to construct something more complete in computational language form. And, unlike what weâ€™ve so far figured out about the innards of ChatGPT, we can expect to design the computational language so that itâ€™s readily understandable to humans.</p>
<p>When we talk about semantic grammar, we can draw an analogy to syllogistic logic. At first, syllogistic logic was essentially a collection of rules about statements expressed in human language. But (yes, two millennia later) <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history" class="external">when formal logic was developed<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, the original basic constructs of syllogistic logic could now be used to build huge â€œformal towersâ€ that include, for example, the operation of modern digital circuitry. And so, we can expect, it will be with more general semantic grammar. At first, it may just be able to deal with simple patterns, expressed, say, as text. But once its whole computational language framework is built, we can expect that it will be able to be used to erect tall towers of â€œgeneralized semantic logicâ€, that allow us to work in a precise and formal way with all sorts of things that have never been accessible to us before, except just at a â€œground-floor levelâ€ through human language, with all its vagueness.</p>
<p>We can think of the construction of computational languageâ€”and semantic grammarâ€”as representing a kind of ultimate compression in representing things. Because it allows us to talk about the essence of whatâ€™s possible, without, for example, dealing with all the â€œturns of phraseâ€ that exist in ordinary human language. And we can view the great strength of ChatGPT as being something a bit similar: because it too has in a sense â€œdrilled throughâ€ to the point where it can â€œput language together in a semantically meaningful wayâ€ without concern for different possible turns of phrase.</p>
<p>So what would happen if we applied ChatGPT to underlying computational language? The computational language can describe whatâ€™s possible. But what can still be added is a sense of â€œwhatâ€™s popularâ€â€”based for example on reading all that content on the web. But thenâ€”underneathâ€”operating with computational language means that something like ChatGPT has immediate and fundamental access to what amount to ultimate tools for making use of potentially irreducible computations. And that makes it a system that can not only â€œgenerate reasonable textâ€, but can expect to work out whatever can be worked out about whether that text actually makes â€œcorrectâ€ statements about the worldâ€”or whatever itâ€™s supposed to be talking about.</p>
<h2 id="so--what-is-chatgpt-doing-and-why-does-it-work">So â€¦ What Is ChatGPT Doing, and Why Does It Work?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#so--what-is-chatgpt-doing-and-why-does-it-work" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>The basic concept of ChatGPT is at some level rather simple. Start from a huge sample of human-created text from the web, books, etc. Then train a neural net to generate text thatâ€™s â€œlike thisâ€. And in particular, make it able to start from a â€œpromptâ€ and then continue with text thatâ€™s â€œlike what itâ€™s been trained withâ€.</p>
<p>As weâ€™ve seen, the actual neural net in ChatGPT is made up of very simple elementsâ€”though billions of them. And the basic operation of the neural net is also very simple, consisting essentially of passing input derived from the text itâ€™s generated so far â€œonce through its elementsâ€ (without any loops, etc.) for every new word (or part of a word) that it generates.</p>
<p>But the remarkableâ€”and unexpectedâ€”thing is that this process can produce text thatâ€™s successfully â€œlikeâ€ whatâ€™s out there on the web, in books, etc. And not only is it coherent human language, it also â€œsays thingsâ€ that â€œfollow its promptâ€ making use of content itâ€™s â€œreadâ€. It doesnâ€™t always say things that â€œglobally make senseâ€ (or correspond to correct computations)â€”because (without, for example, <a href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/" class="external">accessing the â€œcomputational superpowersâ€ of Wolfram|Alpha<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) itâ€™s just saying things that â€œsound rightâ€ based on what things â€œsounded likeâ€ in its training material.</p>
<p>The specific engineering of ChatGPT has made it quite compelling. But ultimately (at least until it can use outside tools) ChatGPT is â€œmerelyâ€ pulling out some â€œcoherent thread of textâ€ from the â€œstatistics of conventional wisdomâ€ that itâ€™s accumulated. But itâ€™s amazing how human-like the results are. And as Iâ€™ve discussed, this suggests something thatâ€™s at least scientifically very important: that human language (and the patterns of thinking behind it) are somehow simpler and more â€œlaw likeâ€ in their structure than we thought. ChatGPT has implicitly discovered it. But we can potentially explicitly expose it, with semantic grammar, computational language, etc.</p>
<p>What ChatGPT does in generating text is very impressiveâ€”and the results are usually very much like what we humans would produce. So does this mean ChatGPT is working like a brain? Its underlying artificial-neural-net structure was ultimately modeled on an idealization of the brain. And it seems quite likely that when we humans generate language many aspects of whatâ€™s going on are quite similar.</p>
<p>When it comes to training (AKA learning) the different â€œhardwareâ€ of the brain and of current computers (as well as, perhaps, some undeveloped algorithmic ideas) forces ChatGPT to use a strategy thatâ€™s probably rather different (and in some ways much less efficient) than the brain. And thereâ€™s something else as well: unlike even in typical algorithmic computation, ChatGPT doesnâ€™t internally â€œhave loopsâ€ or â€œrecompute on dataâ€. And that inevitably limits its computational capabilityâ€”even with respect to current computers, but definitely with respect to the brain.</p>
<p>Itâ€™s not clear how to â€œfix thatâ€ and still maintain the ability to train the system with reasonable efficiency. But to do so will presumably allow a future ChatGPT to do even more â€œbrain-like thingsâ€. Of course, there are plenty of things that brains donâ€™t do so wellâ€”particularly involving what amount to irreducible computations. And for these both brains and things like ChatGPT have to seek â€œoutside toolsâ€â€”like <a href="https://www.wolfram.com/language/" class="external">Wolfram Language<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</p>
<p>But for now itâ€™s exciting to see what ChatGPT has already been able to do. At some level itâ€™s a great example of the fundamental scientific fact that large numbers of simple computational elements can do remarkable and unexpected things. But it also provides perhaps the best impetus weâ€™ve had in two thousand years to understand better just what the fundamental character and principles might be of that central feature of the human condition that is human language and the processes of thinking behind it.</p>
<h2 id="thanks">Thanks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#thanks" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Iâ€™ve been following the development of neural nets now for about 43 years, and during that time Iâ€™ve interacted with many people about them. Among themâ€”some from long ago, some from recently, and some across many yearsâ€”have been: Giulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian Bodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger Germundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun, Jerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce, Tomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon Shaw, Jonas SjÃ¶berg, Ilya Sutskever, Gerry Tesauro and Timothee Verdier. For help with this piece, Iâ€™d particularly like to thank Giulio Alessandrini and Brad Klee.</p>
<h2 id="additional-resources">Additional Resources<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#additional-resources" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2></article></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc" class><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#its-just-adding-one-word-at-a-time" data-for="its-just-adding-one-word-at-a-time">Itâ€™s Just Adding One Word at a Time</a></li><li class="depth-0"><a href="#where-do-the-probabilities-come-from" data-for="where-do-the-probabilities-come-from">Where Do the Probabilities Come From?</a></li><li class="depth-0"><a href="#what-is-a-model" data-for="what-is-a-model">What Is a Model?</a></li><li class="depth-0"><a href="#models-for-human-like-tasks" data-for="models-for-human-like-tasks">Models for Human-Like Tasks</a></li><li class="depth-0"><a href="#neural-nets" data-for="neural-nets">Neural Nets</a></li><li class="depth-0"><a href="#machine-learning-and-the-training-of-neural-nets" data-for="machine-learning-and-the-training-of-neural-nets">Machine Learning, and the Training of Neural Nets</a></li><li class="depth-0"><a href="#the-practice-and-lore-of-neural-net-training" data-for="the-practice-and-lore-of-neural-net-training">The Practice and Lore of Neural Net Training</a></li><li class="depth-0"><a href="#surely-a-network-thats-big-enough-can-do-anything" data-for="surely-a-network-thats-big-enough-can-do-anything">â€œSurely a Network Thatâ€™s Big Enough Can Do Anything!â€</a></li><li class="depth-0"><a href="#the-concept-of-embeddings" data-for="the-concept-of-embeddings">The Concept of Embeddings</a></li><li class="depth-0"><a href="#inside-chatgpt" data-for="inside-chatgpt">Inside ChatGPT</a></li><li class="depth-0"><a href="#the-training-of-chatgpt" data-for="the-training-of-chatgpt">The Training of ChatGPT</a></li><li class="depth-0"><a href="#beyond-basic-training" data-for="beyond-basic-training">Beyond Basic Training</a></li><li class="depth-0"><a href="#what-really-lets-chatgpt-work" data-for="what-really-lets-chatgpt-work">What Really Lets ChatGPT Work?</a></li><li class="depth-0"><a href="#meaning-space-and-semantic-laws-of-motion" data-for="meaning-space-and-semantic-laws-of-motion">Meaning Space and Semantic Laws of Motion</a></li><li class="depth-0"><a href="#semantic-grammar-and-the-power-of-computational-language" data-for="semantic-grammar-and-the-power-of-computational-language">Semantic Grammar and the Power of Computational Language</a></li><li class="depth-0"><a href="#so--what-is-chatgpt-doing-and-why-does-it-work" data-for="so--what-is-chatgpt-doing-and-why-does-it-work">So â€¦ What Is ChatGPT Doing, and Why Does It Work?</a></li><li class="depth-0"><a href="#thanks" data-for="thanks">Thanks</a></li><li class="depth-0"><a href="#additional-resources" data-for="additional-resources">Additional Resources</a></li></ul></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> Â© 2024</p><ul><li><a href="https://github.com/XiGou">GitHub</a></li><li><a href="https://linkedin.com/in/xigou">Linkedin</a></li><li><a href="https://stackoverflow.com/users/10871150/xi-gou">StackOverflow</a></li><li><a href="mailto:gouxivae@gmail.com">Email</a></li></ul></footer></div></body><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="../../../postscript.js" type="module"></script></html>